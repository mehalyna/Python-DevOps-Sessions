{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abf70f61",
      "metadata": {
        "id": "abf70f61"
      },
      "source": [
        "# Python DevOps Session 1: Advanced Automation & Tooling\n",
        "\n",
        "Welcome to our advanced Python DevOps session! This notebook covers practical automation tools and techniques that DevOps engineers use daily.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Build command-line tools with `argparse`\n",
        "- Process large files efficiently with streaming\n",
        "- Create installable Python packages\n",
        "- Handle configuration files (YAML, JSON)\n",
        "- Implement concurrent programming for performance\n",
        "- Work with data formats (CSV, Excel, JSON)\n",
        "- Apply testing best practices with `pytest`\n",
        "\n",
        "## Session Overview\n",
        "1. **Log Analysis** - CLI tools and streaming file processing\n",
        "2. **Package Development** - Creating installable Python packages\n",
        "3. **File Operations** - Configuration-driven file management\n",
        "4. **Data Processing** - CSV to Excel reporting\n",
        "5. **Concurrent Programming** - Async HTTP health checking\n",
        "6. **Text Processing** - Building normalization toolkits\n",
        "\n",
        "## Prerequisites\n",
        "- Completion of Session 0 fundamentals\n",
        "- Understanding of Python functions and classes\n",
        "- Basic command-line experience"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae82644a",
      "metadata": {
        "id": "ae82644a"
      },
      "source": [
        "## Section 1: Command-Line Tools and Streaming File Processing\n",
        "\n",
        "### Theory: Building Professional CLI Tools\n",
        "\n",
        "Command-line interfaces are the backbone of DevOps automation. Key concepts:\n",
        "\n",
        "- **Argument Parsing**: Using `argparse` for professional CLI interfaces\n",
        "- **Streaming Processing**: Handling large files without loading into memory\n",
        "- **File I/O Patterns**: Efficient reading and writing of large datasets\n",
        "- **Error Handling**: Graceful handling of malformed data\n",
        "- **Output Formats**: Supporting multiple output formats (JSON, TSV, CSV)\n",
        "\n",
        "### Essential CLI Methods:\n",
        "- **`argparse.ArgumentParser()`** - Creates command-line argument parser\n",
        "- **`parser.add_argument()`** - Defines command-line options and flags\n",
        "- **`parser.parse_args()`** - Parses command-line arguments\n",
        "- **`open(file, mode)`** - Opens files with different modes ('r', 'w', 'a')\n",
        "- **File iteration** - `for line in file:` for memory-efficient line processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d35b1776",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35b1776",
        "outputId": "b5647765-350c-4f6d-b81d-66fa03ea6317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created sample log with 50 lines: sample_web.log\n",
            "Sample log created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Basic CLI Tool with argparse\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def create_sample_log(filename, num_lines=100):\n",
        "    \"\"\"Create a sample web server log for testing.\"\"\"\n",
        "    sample_entries = [\n",
        "        '192.168.1.100 - - [26/Oct/2025:10:15:30 +0000] \"GET /api/users HTTP/1.1\" 200 1234',\n",
        "        '10.0.0.50 - - [26/Oct/2025:10:16:45 +0000] \"POST /api/login HTTP/1.1\" 401 567',\n",
        "        '192.168.1.200 - - [26/Oct/2025:10:17:12 +0000] \"GET /static/css/main.css HTTP/1.1\" 304 0',\n",
        "        '203.0.113.45 - - [26/Oct/2025:10:18:03 +0000] \"GET /api/data HTTP/1.1\" 500 890',\n",
        "        '192.168.1.100 - - [26/Oct/2025:11:20:15 +0000] \"DELETE /api/user/123 HTTP/1.1\" 204 0'\n",
        "    ]\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        for i in range(num_lines):\n",
        "            # Vary the entries and timestamps\n",
        "            entry = sample_entries[i % len(sample_entries)]\n",
        "            f.write(entry + '\\n')\n",
        "\n",
        "    print(f\"Created sample log with {num_lines} lines: {filename}\")\n",
        "\n",
        "# Create a sample log file for demonstration\n",
        "create_sample_log('sample_web.log', 50)\n",
        "print(\"Sample log created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c9dc0c",
      "metadata": {
        "id": "c3c9dc0c",
        "outputId": "95e8c243-55b7-4584-e27f-a1d1d5dbb494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LOG ANALYSIS REPORT ===\n",
            "Total Requests: 50\n",
            "Status Distribution: {'2xx': 20, '3xx': 10, '4xx': 10, '5xx': 10}\n",
            "\n",
            "Top Client IPs: {'192.168.1.100': 20, '10.0.0.50': 10, '192.168.1.200': 10, '203.0.113.45': 10}\n",
            "Top Paths: {'/api/users': 10, '/api/login': 10, '/static/css/main.css': 10, '/api/data': 10, '/api/user/123': 10}\n",
            "Hourly Distribution: {'10': 40, '11': 10}\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Streaming Log Parser with CLI\n",
        "class LogAnalyzer:\n",
        "    \"\"\"A streaming log analyzer that processes web server logs efficiently.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stats = {\n",
        "            'total_requests': 0,\n",
        "            'status_codes': {'2xx': 0, '3xx': 0, '4xx': 0, '5xx': 0},\n",
        "            'client_ips': {},\n",
        "            'paths': {},\n",
        "            'hourly_counts': {}\n",
        "        }\n",
        "\n",
        "    def parse_log_line(self, line):\n",
        "        \"\"\"Parse a single log line in Common Log Format.\"\"\"\n",
        "        # Common Log Format pattern\n",
        "        pattern = r'(\\S+) \\S+ \\S+ \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) \\S+\" (\\d{3}) (\\d+|-)'\n",
        "\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if not match:\n",
        "            return None\n",
        "\n",
        "        ip, timestamp, method, path, status, size = match.groups()\n",
        "\n",
        "        return {\n",
        "            'ip': ip,\n",
        "            'timestamp': timestamp,\n",
        "            'method': method,\n",
        "            'path': path,\n",
        "            'status': int(status),\n",
        "            'size': int(size) if size != '-' else 0\n",
        "        }\n",
        "\n",
        "    def update_stats(self, parsed_line):\n",
        "        \"\"\"Update statistics with parsed log entry.\"\"\"\n",
        "        if not parsed_line:\n",
        "            return\n",
        "\n",
        "        # Total requests\n",
        "        self.stats['total_requests'] += 1\n",
        "\n",
        "        # Status code categories\n",
        "        status = parsed_line['status']\n",
        "        if 200 <= status < 300:\n",
        "            self.stats['status_codes']['2xx'] += 1\n",
        "        elif 300 <= status < 400:\n",
        "            self.stats['status_codes']['3xx'] += 1\n",
        "        elif 400 <= status < 500:\n",
        "            self.stats['status_codes']['4xx'] += 1\n",
        "        elif 500 <= status < 600:\n",
        "            self.stats['status_codes']['5xx'] += 1\n",
        "\n",
        "        # Client IPs\n",
        "        ip = parsed_line['ip']\n",
        "        self.stats['client_ips'][ip] = self.stats['client_ips'].get(ip, 0) + 1\n",
        "\n",
        "        # Request paths\n",
        "        path = parsed_line['path']\n",
        "        self.stats['paths'][path] = self.stats['paths'].get(path, 0) + 1\n",
        "\n",
        "        # Hourly counts (extract hour from timestamp)\n",
        "        timestamp = parsed_line['timestamp']\n",
        "        hour = timestamp.split(':')[1]  # Extract hour from timestamp but better to use datetime.strptime() or dateutil.parser.parser()\n",
        "        self.stats['hourly_counts'][hour] = self.stats['hourly_counts'].get(hour, 0) + 1\n",
        "\n",
        "    def process_file(self, filename):\n",
        "        \"\"\"Process log file line by line (streaming).\"\"\"\n",
        "        malformed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(filename, 'r') as file:\n",
        "                for line_num, line in enumerate(file, 1):\n",
        "                    parsed = self.parse_log_line(line)\n",
        "                    if parsed is None:\n",
        "                        malformed_lines += 1\n",
        "                        print(f\"Warning: Malformed line {line_num}: {line.strip()}\")\n",
        "                        continue\n",
        "\n",
        "                    self.update_stats(parsed)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File {filename} not found\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {e}\")\n",
        "            return False\n",
        "\n",
        "        if malformed_lines > 0:\n",
        "            print(f\"Processed file with {malformed_lines} malformed lines\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_top_n(self, data_dict, n=10):\n",
        "        \"\"\"Get top N items from a dictionary by value.\"\"\"\n",
        "        return sorted(data_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "    def generate_report(self, top_n=10):\n",
        "        \"\"\"Generate comprehensive report.\"\"\"\n",
        "        return {\n",
        "            'summary': {\n",
        "                'total_requests': self.stats['total_requests'],\n",
        "                'status_distribution': self.stats['status_codes']\n",
        "            },\n",
        "            'top_client_ips': dict(self.get_top_n(self.stats['client_ips'], top_n)),\n",
        "            'top_paths': dict(self.get_top_n(self.stats['paths'], top_n)),\n",
        "            'hourly_distribution': self.stats['hourly_counts']\n",
        "        }\n",
        "\n",
        "# Test the log analyzer\n",
        "analyzer = LogAnalyzer()\n",
        "success = analyzer.process_file('sample_web.log')\n",
        "\n",
        "if success:\n",
        "    report = analyzer.generate_report(top_n=5)\n",
        "    print(\"\\n=== LOG ANALYSIS REPORT ===\")\n",
        "    print(f\"Total Requests: {report['summary']['total_requests']}\")\n",
        "    print(f\"Status Distribution: {report['summary']['status_distribution']}\")\n",
        "    print(f\"\\nTop Client IPs: {report['top_client_ips']}\")\n",
        "    print(f\"Top Paths: {report['top_paths']}\")\n",
        "    print(f\"Hourly Distribution: {report['hourly_distribution']}\")\n",
        "else:\n",
        "    print(\"Failed to process log file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e82e25",
      "metadata": {
        "id": "c1e82e25"
      },
      "source": [
        "### Key Methods Explained:\n",
        "\n",
        "**File Processing Methods:**\n",
        "- **`open(filename, 'r')`** - Opens file for reading in text mode\n",
        "- **`enumerate(file, 1)`** - Provides line numbers starting from 1 for each line\n",
        "- **`for line in file:`** - Iterates through file line by line (memory efficient)\n",
        "- **`line.strip()`** - Removes leading/trailing whitespace including newlines\n",
        "\n",
        "**Regular Expression Methods:**\n",
        "- **`re.match(pattern, string)`** - Matches pattern at the beginning of string\n",
        "- **`match.groups()`** - Returns captured groups from regex match\n",
        "- **Common Log Format** - Standard web server log format with IP, timestamp, request, status\n",
        "\n",
        "**Dictionary Methods for Aggregation:**\n",
        "- **`dict.get(key, default)`** - Returns value for key, or default if key doesn't exist\n",
        "- **`sorted(dict.items(), key=lambda x: x[1], reverse=True)`** - Sorts dictionary by values descending\n",
        "\n",
        "**Exception Handling:**\n",
        "- **`try/except/finally`** - Handles file operations and parsing errors gracefully\n",
        "- **`FileNotFoundError`** - Specific exception for missing files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "862f56c6",
      "metadata": {
        "id": "862f56c6"
      },
      "source": [
        "## Section 2: Python Package Development and Virtual Environments\n",
        "\n",
        "### Theory: Creating Installable Packages\n",
        "\n",
        "Professional Python development requires understanding:\n",
        "\n",
        "- **Package Structure**: Organizing code into installable packages\n",
        "- **Virtual Environments**: Isolating dependencies for reproducible builds\n",
        "- **Configuration Files**: Working with YAML, JSON for application config\n",
        "- **Entry Points**: Creating command-line tools from packages\n",
        "- **Testing**: Using pytest for comprehensive testing\n",
        "\n",
        "### Essential Package Development Methods:\n",
        "- **`setup.py`** - Traditional package configuration\n",
        "- **`pyproject.toml`** - Modern package configuration (PEP 518)\n",
        "- **`pip install -e .`** - Install package in development mode\n",
        "- **`yaml.safe_load()`** - Parse YAML configuration files safely\n",
        "- **`json.load()`** - Parse JSON configuration files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0391ba0c",
      "metadata": {
        "id": "0391ba0c",
        "outputId": "41bdbb68-72db-4d83-d6e3-8728535eccbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded successfully\n",
            "✓ Configuration validated successfully\n",
            "\n",
            "=== CONFIGURATION SUMMARY ===\n",
            "{\n",
            "  \"application\": {\n",
            "    \"name\": \"MyApp\",\n",
            "    \"version\": \"1.2.3\"\n",
            "  },\n",
            "  \"services\": {\n",
            "    \"total\": 4,\n",
            "    \"enabled\": 3,\n",
            "    \"disabled\": 1,\n",
            "    \"enabled_list\": [\n",
            "      \"web-server\",\n",
            "      \"database\",\n",
            "      \"api-gateway\"\n",
            "    ],\n",
            "    \"disabled_list\": [\n",
            "      \"cache\"\n",
            "    ]\n",
            "  },\n",
            "  \"ports_in_use\": [\n",
            "    8080,\n",
            "    5432,\n",
            "    3000\n",
            "  ]\n",
            "}\n",
            "\n",
            "=== TESTING ERROR HANDLING ===\n",
            "✓ Correctly caught validation error: Configuration validation failed: Missing required field: services\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Configuration Tool Package\n",
        "import yaml\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "class ConfigValidator:\n",
        "    \"\"\"Validates and processes YAML configuration files.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.required_fields = ['name', 'version', 'services']\n",
        "        self.valid_service_fields = ['name', 'port', 'enabled']\n",
        "\n",
        "    def load_config(self, config_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load configuration from YAML file.\"\"\"\n",
        "        if not os.path.exists(config_path):\n",
        "            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
        "\n",
        "        try:\n",
        "            with open(config_path, 'r') as file:\n",
        "                config = yaml.safe_load(file)\n",
        "                if config is None:\n",
        "                    raise ValueError(\"Configuration file is empty or invalid\")\n",
        "                return config\n",
        "        except yaml.YAMLError as e:\n",
        "            raise ValueError(f\"Invalid YAML format: {e}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error reading configuration: {e}\")\n",
        "\n",
        "    def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Validate configuration structure and content.\"\"\"\n",
        "        errors = []\n",
        "\n",
        "        # Check required fields\n",
        "        for field in self.required_fields:\n",
        "            if field not in config:\n",
        "                errors.append(f\"Missing required field: {field}\")\n",
        "\n",
        "        if errors:\n",
        "            raise ValueError(f\"Configuration validation failed: {', '.join(errors)}\")\n",
        "\n",
        "        # Validate name\n",
        "        if not isinstance(config['name'], str) or not config['name'].strip():\n",
        "            raise ValueError(\"Field 'name' must be a non-empty string\")\n",
        "\n",
        "        # Validate version\n",
        "        if not isinstance(config['version'], str):\n",
        "            raise ValueError(\"Field 'version' must be a string\")\n",
        "\n",
        "        # Validate services\n",
        "        if not isinstance(config['services'], list):\n",
        "            raise ValueError(\"Field 'services' must be a list\")\n",
        "\n",
        "        validated_services = []\n",
        "        for i, service in enumerate(config['services']):\n",
        "            if not isinstance(service, dict):\n",
        "                raise ValueError(f\"Service {i} must be a dictionary\")\n",
        "\n",
        "            validated_service = self._validate_service(service, i)\n",
        "            validated_services.append(validated_service)\n",
        "\n",
        "        return {\n",
        "            'name': config['name'].strip(),\n",
        "            'version': config['version'].strip(),\n",
        "            'services': validated_services\n",
        "        }\n",
        "\n",
        "    def _validate_service(self, service: Dict[str, Any], index: int) -> Dict[str, Any]:\n",
        "        \"\"\"Validate individual service configuration.\"\"\"\n",
        "        if 'name' not in service:\n",
        "            raise ValueError(f\"Service {index}: missing required field 'name'\")\n",
        "\n",
        "        validated = {\n",
        "            'name': str(service['name']).strip(),\n",
        "            'port': service.get('port', 8080), # int(service.get('port', 8080))\n",
        "            'enabled': service.get('enabled', True)\n",
        "        }\n",
        "\n",
        "        # Validate port\n",
        "        if not isinstance(validated['port'], int) or not (1 <= validated['port'] <= 65535):\n",
        "            raise ValueError(f\"Service {index}: port must be an integer between 1 and 65535\")\n",
        "\n",
        "        # Validate enabled\n",
        "        if not isinstance(validated['enabled'], bool):\n",
        "            raise ValueError(f\"Service {index}: enabled must be a boolean\")\n",
        "\n",
        "        return validated\n",
        "\n",
        "    def generate_summary(self, config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generate a summary of the validated configuration.\"\"\"\n",
        "        enabled_services = [s for s in config['services'] if s['enabled']]\n",
        "        disabled_services = [s for s in config['services'] if not s['enabled']]\n",
        "\n",
        "        return {\n",
        "            'application': {\n",
        "                'name': config['name'],\n",
        "                'version': config['version']\n",
        "            },\n",
        "            'services': {\n",
        "                'total': len(config['services']),\n",
        "                'enabled': len(enabled_services),\n",
        "                'disabled': len(disabled_services),\n",
        "                'enabled_list': [s['name'] for s in enabled_services],\n",
        "                'disabled_list': [s['name'] for s in disabled_services]\n",
        "            },\n",
        "            'ports_in_use': [s['port'] for s in enabled_services]\n",
        "        }\n",
        "\n",
        "# Create a sample configuration file\n",
        "sample_config = {\n",
        "    'name': 'MyApp',\n",
        "    'version': '1.2.3',\n",
        "    'services': [\n",
        "        {'name': 'web-server', 'port': 8080, 'enabled': True},\n",
        "        {'name': 'database', 'port': 5432, 'enabled': True},\n",
        "        {'name': 'cache', 'port': 6379, 'enabled': False},\n",
        "        {'name': 'api-gateway', 'port': 3000, 'enabled': True}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Write sample config to file\n",
        "with open('sample_config.yml', 'w') as f:\n",
        "    yaml.dump(sample_config, f, default_flow_style=False)\n",
        "\n",
        "# Test the configuration validator\n",
        "validator = ConfigValidator()\n",
        "\n",
        "try:\n",
        "    # Load and validate configuration\n",
        "    config = validator.load_config('sample_config.yml')\n",
        "    print(\"✓ Configuration loaded successfully\")\n",
        "\n",
        "    validated_config = validator.validate_config(config)\n",
        "    print(\"✓ Configuration validated successfully\")\n",
        "\n",
        "    # Generate summary\n",
        "    summary = validator.generate_summary(validated_config)\n",
        "    print(\"\\n=== CONFIGURATION SUMMARY ===\")\n",
        "    print(json.dumps(summary, indent=2))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Configuration error: {e}\")\n",
        "\n",
        "# Test with invalid configuration\n",
        "print(\"\\n=== TESTING ERROR HANDLING ===\")\n",
        "invalid_config = {'name': '', 'version': 123}  # Missing services, invalid types\n",
        "\n",
        "try:\n",
        "    validator.validate_config(invalid_config)\n",
        "except ValueError as e:\n",
        "    print(f\"✓ Correctly caught validation error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53f6fff",
      "metadata": {
        "id": "f53f6fff"
      },
      "source": [
        "### Key Package Development Methods:\n",
        "\n",
        "**YAML Processing:**\n",
        "- **`yaml.safe_load(file)`** - Safely parse YAML content from file object\n",
        "- **`yaml.dump(data, file)`** - Write Python data structures to YAML format\n",
        "- **`yaml.YAMLError`** - Exception for YAML parsing errors\n",
        "\n",
        "**Configuration Validation:**\n",
        "- **`isinstance(obj, type)`** - Check if object is of specific type\n",
        "- **`dict.get(key, default)`** - Get dictionary value with fallback default\n",
        "- **Type hints** - `Dict[str, Any]`, `List[str]` for better code documentation\n",
        "\n",
        "**File Operations:**\n",
        "- **`os.path.exists(path)`** - Check if file or directory exists\n",
        "- **`with open(file, mode) as f:`** - Context manager for safe file handling\n",
        "- **Exception chaining** - Raising new exceptions with original context\n",
        "\n",
        "**Package Structure Concepts:**\n",
        "- **Entry points** - Command-line interfaces defined in setup.py\n",
        "- **Development install** - `pip install -e .` for live code updates\n",
        "- **Requirements.txt** - Dependency specification for reproducible environments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b7f5cb",
      "metadata": {
        "id": "d7b7f5cb"
      },
      "source": [
        "## Section 3: Configuration-Driven File Operations\n",
        "\n",
        "### Theory: Safe File Management\n",
        "\n",
        "File operations are critical in DevOps for:\n",
        "- **Configuration management** - Updating config files across environments\n",
        "- **Log rotation** - Managing log file naming and archival\n",
        "- **Backup operations** - Systematic file organization\n",
        "- **Deployment preparation** - File staging and organization\n",
        "\n",
        "### Essential File Operation Methods:\n",
        "- **`glob.glob(pattern)`** - Find files matching wildcard patterns\n",
        "- **`os.path.join()`** - Platform-independent path construction\n",
        "- **`shutil.move(src, dst)`** - Move/rename files and directories\n",
        "- **`pathlib.Path`** - Modern object-oriented path handling\n",
        "- **Pattern matching** - Regular expressions and glob patterns for file selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7ee1560",
      "metadata": {
        "id": "f7ee1560",
        "outputId": "2c8788b9-b653-4ab2-f831-9d00dd7c9c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 7 test files in test_files\n",
            "\n",
            "Initial files in test_files:\n",
            "  - application.log\n",
            "  - config.backup\n",
            "  - database.backup\n",
            "  - error.log\n",
            "  - readme.md\n",
            "  - rename_manifest.json\n",
            "  - temp_data.txt\n",
            "  - temp_report.txt\n",
            "\n",
            "=== DRY RUN for directory: test_files ===\n",
            "\n",
            "Rule: Add date prefix to log files\n",
            "Pattern: *.log -> {date}_{orig}\n",
            "  application.log -> 20251202_application.log\n",
            "  error.log -> 20251202_error.log\n",
            "\n",
            "Rule: Rename temp files to processed\n",
            "Pattern: temp_*.txt -> processed_{orig}\n",
            "  temp_data.txt -> processed_temp_data.txt\n",
            "  temp_report.txt -> processed_temp_report.txt\n",
            "\n",
            "Rule: Add timestamp to backup files\n",
            "Pattern: *.backup -> backup_{datetime}_{orig}\n",
            "  config.backup -> backup_20251202_112959_config.backup\n",
            "  database.backup -> backup_20251202_112959_database.backup\n",
            "\n",
            " Proceeding with renames...\n",
            "\n",
            "=== COMMITTING 6 RENAMES ===\n",
            "application.log -> 20251202_application.log\n",
            "error.log -> 20251202_error.log\n",
            "temp_data.txt -> processed_temp_data.txt\n",
            "temp_report.txt -> processed_temp_report.txt\n",
            "config.backup -> backup_20251202_112959_config.backup\n",
            "database.backup -> backup_20251202_112959_database.backup\n",
            "\n",
            "Cleaning up previously renamed files...\n",
            "  Removed: backup_20251202_112959_config.backup\n",
            "  Removed: backup_20251202_112959_database.backup\n",
            "  Removed: processed_temp_data.txt\n",
            "  Removed: processed_temp_report.txt\n",
            "\n",
            "Recreating original test files...\n",
            "Created 7 test files in test_files\n",
            "\n",
            " Manifest saved: test_files\\rename_manifest.json\n",
            "\n",
            "Final files in test_files:\n",
            "  - 20251202_application.log\n",
            "  - 20251202_error.log\n",
            "  - application.log\n",
            "  - config.backup\n",
            "  - database.backup\n",
            "  - error.log\n",
            "  - readme.md\n",
            "   rename_manifest.json (manifest)\n",
            "  - temp_data.txt\n",
            "  - temp_report.txt\n"
          ]
        }
      ],
      "source": [
        "# Example 4: Configuration-Driven File Renamer\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class FileRenamer:\n",
        "    \"\"\"Safely rename files based on YAML configuration rules.\"\"\"\n",
        "\n",
        "    def __init__(self, config_path: str):\n",
        "        self.config = self._load_config(config_path)\n",
        "        self.rename_manifest = []\n",
        "\n",
        "    def _load_config(self, config_path: str) -> Dict:\n",
        "        \"\"\"Load rename configuration from YAML file.\"\"\"\n",
        "        # For this example, we'll use a Python dict instead of YAML\n",
        "        # In practice, you'd use yaml.safe_load()\n",
        "        sample_config = {\n",
        "            'rules': [\n",
        "                {\n",
        "                    'pattern': '*.log',\n",
        "                    'template': '{date}_{orig}',\n",
        "                    'description': 'Add date prefix to log files'\n",
        "                },\n",
        "                {\n",
        "                    'pattern': 'temp_*.txt',\n",
        "                    'template': 'processed_{orig}',\n",
        "                    'description': 'Rename temp files to processed'\n",
        "                },\n",
        "                {\n",
        "                    'pattern': '*.backup',\n",
        "                    'template': 'backup_{datetime}_{orig}',\n",
        "                    'description': 'Add timestamp to backup files'\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        return sample_config\n",
        "\n",
        "    def create_test_files(self, directory: str):\n",
        "        \"\"\"Create sample files for testing.\"\"\"\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        test_files = [\n",
        "            'application.log',\n",
        "            'error.log',\n",
        "            'temp_data.txt',\n",
        "            'temp_report.txt',\n",
        "            'database.backup',\n",
        "            'config.backup',\n",
        "            'readme.md'  # This won't match any pattern\n",
        "        ]\n",
        "\n",
        "        for filename in test_files:\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'w') as f:\n",
        "                f.write(f\"Sample content for {filename}\\n\")\n",
        "\n",
        "        print(f\"Created {len(test_files)} test files in {directory}\")\n",
        "        return test_files\n",
        "\n",
        "    def find_matching_files(self, directory: str, pattern: str) -> List[str]:\n",
        "        \"\"\"Find files matching the given glob pattern.\"\"\"\n",
        "        search_pattern = os.path.join(directory, pattern)\n",
        "        return glob.glob(search_pattern)\n",
        "\n",
        "    def generate_new_filename(self, original_path: str, template: str) -> str:\n",
        "        \"\"\"Generate new filename based on template variables.\"\"\"\n",
        "        path_obj = Path(original_path)\n",
        "        original_name = path_obj.stem  # filename without extension\n",
        "        extension = path_obj.suffix    # file extension\n",
        "        parent = path_obj.parent.name  # parent directory name\n",
        "\n",
        "        # Template variables\n",
        "        variables = {\n",
        "            'orig': path_obj.name,  # original filename with extension\n",
        "            'name': original_name,  # filename without extension\n",
        "            'ext': extension.lstrip('.'),  # extension without dot\n",
        "            'parent': parent,\n",
        "            'date': datetime.now().strftime('%Y%m%d'),\n",
        "            'datetime': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        }\n",
        "\n",
        "        # Replace template variables\n",
        "        new_name = template\n",
        "        for var, value in variables.items():\n",
        "            new_name = new_name.replace(f'{{{var}}}', str(value))\n",
        "\n",
        "        # Return full path with new filename\n",
        "        return os.path.join(path_obj.parent, new_name)\n",
        "\n",
        "    def check_conflicts(self, rename_operations: List[Tuple[str, str]]) -> List[str]:\n",
        "        \"\"\"Check for naming conflicts in rename operations.\"\"\"\n",
        "        conflicts = []\n",
        "        target_files = set()\n",
        "\n",
        "        for old_path, new_path in rename_operations:\n",
        "            if new_path in target_files:\n",
        "                conflicts.append(f\"Duplicate target: {new_path}\")\n",
        "\n",
        "            if os.path.exists(new_path):\n",
        "                conflicts.append(f\"Target exists: {new_path}\")\n",
        "\n",
        "            target_files.add(new_path)\n",
        "\n",
        "        return conflicts\n",
        "\n",
        "    def dry_run(self, directory: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Perform dry run to show what would be renamed.\"\"\"\n",
        "        rename_operations = []\n",
        "\n",
        "        print(f\"\\n=== DRY RUN for directory: {directory} ===\")\n",
        "\n",
        "        for rule in self.config['rules']:\n",
        "            pattern = rule['pattern']\n",
        "            template = rule['template']\n",
        "            description = rule['description']\n",
        "\n",
        "            matching_files = self.find_matching_files(directory, pattern)\n",
        "\n",
        "            if matching_files:\n",
        "                print(f\"\\nRule: {description}\")\n",
        "                print(f\"Pattern: {pattern} -> {template}\")\n",
        "\n",
        "                for file_path in matching_files:\n",
        "                    new_path = self.generate_new_filename(file_path, template)\n",
        "                    rename_operations.append((file_path, new_path))\n",
        "                    print(f\"  {os.path.basename(file_path)} -> {os.path.basename(new_path)}\")\n",
        "\n",
        "        # Check for conflicts\n",
        "        conflicts = self.check_conflicts(rename_operations)\n",
        "        if conflicts:\n",
        "            print(f\"\\n  CONFLICTS DETECTED:\")\n",
        "            for conflict in conflicts:\n",
        "                print(f\"  - {conflict}\")\n",
        "\n",
        "        return rename_operations\n",
        "\n",
        "    def commit_renames(self, rename_operations: List[Tuple[str, str]], force: bool = False):\n",
        "        \"\"\"Perform actual file renames.\"\"\"\n",
        "        if not force:\n",
        "            conflicts = self.check_conflicts(rename_operations)\n",
        "            if conflicts:\n",
        "                raise ValueError(f\"Conflicts detected. Use --force to override: {conflicts}\")\n",
        "\n",
        "        print(f\"\\n=== COMMITTING {len(rename_operations)} RENAMES ===\")\n",
        "\n",
        "        for old_path, new_path in rename_operations:\n",
        "            try:\n",
        "                shutil.move(old_path, new_path)\n",
        "\n",
        "                # Record in manifest\n",
        "                self.rename_manifest.append({\n",
        "                    'original': old_path,\n",
        "                    'renamed': new_path,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                print(f\"{os.path.basename(old_path)} -> {os.path.basename(new_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to rename {old_path}: {e}\")\n",
        "\n",
        "    def save_manifest(self, directory: str):\n",
        "        \"\"\"Save rename manifest for rollback capability.\"\"\"\n",
        "        manifest_path = os.path.join(directory, 'rename_manifest.json')\n",
        "\n",
        "        with open(manifest_path, 'w') as f:\n",
        "            json.dump(self.rename_manifest, f, indent=2)\n",
        "\n",
        "        print(f\"\\n Manifest saved: {manifest_path}\")\n",
        "\n",
        "# Test the file renamer\n",
        "test_dir = 'test_files'\n",
        "renamer = FileRenamer('rename_config.yml')\n",
        "\n",
        "# Create test files\n",
        "created_files = renamer.create_test_files(test_dir)\n",
        "\n",
        "# Show initial files\n",
        "print(f\"\\nInitial files in {test_dir}:\")\n",
        "for f in os.listdir(test_dir):\n",
        "    print(f\"  - {f}\")\n",
        "\n",
        "# Perform dry run\n",
        "rename_ops = renamer.dry_run(test_dir)\n",
        "\n",
        "# Simulate committing renames (in practice, you'd have CLI flags)\n",
        "print(f\"\\n Proceeding with renames...\")\n",
        "renamer.commit_renames(rename_ops, force=True)\n",
        "# Clean up any previously renamed files to avoid conflicts\n",
        "print(f\"\\nCleaning up previously renamed files...\")\n",
        "for file in os.listdir(test_dir):\n",
        "    filepath = os.path.join(test_dir, file)\n",
        "    if file.startswith('20251026_') or file.startswith('processed_') or file.startswith('backup_'):\n",
        "        os.remove(filepath)\n",
        "        print(f\"  Removed: {file}\")\n",
        "\n",
        "# Recreate original test files\n",
        "print(f\"\\nRecreating original test files...\")\n",
        "renamer.create_test_files(test_dir)\n",
        "# Save manifest\n",
        "renamer.save_manifest(test_dir)\n",
        "\n",
        "# Show final files\n",
        "print(f\"\\nFinal files in {test_dir}:\")\n",
        "for f in os.listdir(test_dir):\n",
        "    if f.endswith('.json'):\n",
        "        print(f\"   {f} (manifest)\")\n",
        "    else:\n",
        "        print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee44ecf6",
      "metadata": {
        "id": "ee44ecf6"
      },
      "source": [
        "### Key File Operation Methods:\n",
        "\n",
        "**Path Manipulation:**\n",
        "- **`pathlib.Path(path)`** - Modern object-oriented path handling\n",
        "- **`path.stem`** - Filename without extension\n",
        "- **`path.suffix`** - File extension including dot\n",
        "- **`path.parent`** - Parent directory of the file\n",
        "- **`os.path.join(dir, filename)`** - Platform-independent path joining\n",
        "\n",
        "**File Pattern Matching:**\n",
        "- **`glob.glob(pattern)`** - Find files matching Unix shell-style wildcards\n",
        "- **Glob patterns**: `*.log` (all .log files), `temp_*` (files starting with temp_)\n",
        "- **Template variables**: `{date}`, `{datetime}`, `{orig}` for dynamic naming\n",
        "\n",
        "**Safe File Operations:**\n",
        "- **`shutil.move(src, dst)`** - Move/rename files with error handling\n",
        "- **`os.makedirs(path, exist_ok=True)`** - Create directories recursively\n",
        "- **Conflict detection** - Check for existing files before rename operations\n",
        "- **Manifest generation** - Record operations for rollback capability\n",
        "\n",
        "**String Formatting:**\n",
        "- **`str.replace(old, new)`** - Replace template variables with actual values\n",
        "- **`datetime.strftime(format)`** - Format timestamps for filenames\n",
        "- **`str.lstrip('.')`** - Remove leading characters (dots from extensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93e48eb9",
      "metadata": {},
      "source": [
        "### **Example 4.1: Handling Locked Files with Retry Logic and Queuing**\n",
        "\n",
        "When files are locked by other processes (open in Excel, being written to, etc.), we need strategies to handle these gracefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b9485c2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 12:25:58,826 - INFO - Worker thread started (Thread-26932)\n",
            "2025-12-02 12:25:58,829 - INFO - Worker thread started (Thread-30796)\n",
            "2025-12-02 12:25:58,830 - INFO - Started 2 worker threads\n",
            "2025-12-02 12:25:58,829 - INFO - Worker thread started (Thread-30796)\n",
            "2025-12-02 12:25:58,830 - INFO - Started 2 worker threads\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 12:25:58,838 - INFO - Queued operation: op_0\n",
            "2025-12-02 12:25:58,839 - INFO - Processing operation: op_0\n",
            "2025-12-02 12:25:58,839 - INFO - Queued operation: op_1\n",
            "2025-12-02 12:25:58,839 - INFO - Processing operation: op_0\n",
            "2025-12-02 12:25:58,839 - INFO - Queued operation: op_1\n",
            "2025-12-02 12:25:58,840 - INFO - Processing operation: op_1\n",
            "2025-12-02 12:25:58,840 - INFO - Attempt 1/3 for op_0\n",
            "2025-12-02 12:25:58,841 - INFO - Queued operation: op_2\n",
            "2025-12-02 12:25:58,843 - INFO - Attempt 1/3 for op_1\n",
            "2025-12-02 12:25:58,846 - INFO - Queued operation: op_3\n",
            "2025-12-02 12:25:58,850 - INFO - Queued operation: op_4\n",
            "2025-12-02 12:25:58,840 - INFO - Processing operation: op_1\n",
            "2025-12-02 12:25:58,840 - INFO - Attempt 1/3 for op_0\n",
            "2025-12-02 12:25:58,841 - INFO - Queued operation: op_2\n",
            "2025-12-02 12:25:58,843 - INFO - Attempt 1/3 for op_1\n",
            "2025-12-02 12:25:58,846 - INFO - Queued operation: op_3\n",
            "2025-12-02 12:25:58,850 - INFO - Queued operation: op_4\n",
            "2025-12-02 12:25:58,850 - INFO - Successfully moved locked_files_test\\document_0.txt -> locked_files_test\\renamed_document_0.txt\n",
            "2025-12-02 12:25:58,850 - INFO - Successfully moved locked_files_test\\document_0.txt -> locked_files_test\\renamed_document_0.txt\n",
            "2025-12-02 12:25:58,856 - INFO - ✓ Operation op_0 completed successfully\n",
            "2025-12-02 12:25:58,857 - INFO - Successfully moved locked_files_test\\document_1.txt -> locked_files_test\\renamed_document_1.txt\n",
            "2025-12-02 12:25:58,857 - INFO - Processing operation: op_2\n",
            "2025-12-02 12:25:58,858 - INFO - ✓ Operation op_1 completed successfully\n",
            "2025-12-02 12:25:58,859 - INFO - Attempt 1/3 for op_2\n",
            "2025-12-02 12:25:58,860 - INFO - Processing operation: op_3\n",
            "2025-12-02 12:25:58,863 - INFO - Attempt 1/3 for op_3\n",
            "2025-12-02 12:25:58,856 - INFO - ✓ Operation op_0 completed successfully\n",
            "2025-12-02 12:25:58,857 - INFO - Successfully moved locked_files_test\\document_1.txt -> locked_files_test\\renamed_document_1.txt\n",
            "2025-12-02 12:25:58,857 - INFO - Processing operation: op_2\n",
            "2025-12-02 12:25:58,858 - INFO - ✓ Operation op_1 completed successfully\n",
            "2025-12-02 12:25:58,859 - INFO - Attempt 1/3 for op_2\n",
            "2025-12-02 12:25:58,860 - INFO - Processing operation: op_3\n",
            "2025-12-02 12:25:58,863 - INFO - Attempt 1/3 for op_3\n",
            "2025-12-02 12:25:58,868 - INFO - Successfully moved locked_files_test\\document_3.txt -> locked_files_test\\renamed_document_3.txt\n",
            "2025-12-02 12:25:58,870 - ERROR - ✗ Operation op_2 failed: Permission denied: [WinError 32] The process cannot access the file because it is being used by another process: 'locked_files_test\\\\document_2.txt'\n",
            "2025-12-02 12:25:58,870 - INFO - ✓ Operation op_3 completed successfully\n",
            "2025-12-02 12:25:58,871 - INFO - Processing operation: op_4\n",
            "2025-12-02 12:25:58,873 - INFO - Attempt 1/3 for op_4\n",
            "2025-12-02 12:25:58,877 - INFO - Successfully moved locked_files_test\\document_4.txt -> locked_files_test\\renamed_document_4.txt\n",
            "2025-12-02 12:25:58,877 - INFO - ✓ Operation op_4 completed successfully\n",
            "2025-12-02 12:25:58,868 - INFO - Successfully moved locked_files_test\\document_3.txt -> locked_files_test\\renamed_document_3.txt\n",
            "2025-12-02 12:25:58,870 - ERROR - ✗ Operation op_2 failed: Permission denied: [WinError 32] The process cannot access the file because it is being used by another process: 'locked_files_test\\\\document_2.txt'\n",
            "2025-12-02 12:25:58,870 - INFO - ✓ Operation op_3 completed successfully\n",
            "2025-12-02 12:25:58,871 - INFO - Processing operation: op_4\n",
            "2025-12-02 12:25:58,873 - INFO - Attempt 1/3 for op_4\n",
            "2025-12-02 12:25:58,877 - INFO - Successfully moved locked_files_test\\document_4.txt -> locked_files_test\\renamed_document_4.txt\n",
            "2025-12-02 12:25:58,877 - INFO - ✓ Operation op_4 completed successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LOCKED FILE HANDLER DEMONSTRATION ===\n",
            "\n",
            "Created 5 test files\n",
            "\n",
            "Queueing file operations...\n",
            "\n",
            "\n",
            " Simulating locked file scenario...\n",
            "Opening document_2.txt to lock it...\n",
            "✓ File document_2.txt is now locked\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 12:26:03,855 - INFO - Waiting for all operations to complete...\n",
            "2025-12-02 12:26:03,856 - INFO - All operations completed\n",
            "2025-12-02 12:26:03,856 - INFO - All operations completed\n",
            "2025-12-02 12:26:03,893 - INFO - Worker thread stopped (Thread-30796)\n",
            "2025-12-02 12:26:03,893 - INFO - Worker thread stopped (Thread-26932)\n",
            "2025-12-02 12:26:03,896 - INFO - All workers stopped\n",
            "2025-12-02 12:26:03,893 - INFO - Worker thread stopped (Thread-30796)\n",
            "2025-12-02 12:26:03,893 - INFO - Worker thread stopped (Thread-26932)\n",
            "2025-12-02 12:26:03,896 - INFO - All workers stopped\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Unlocking file...\n",
            "✓ File unlocked\n",
            "\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Total Operations: 5\n",
            "Successful: 4\n",
            "Locked: 0\n",
            "Failed: 0\n",
            "Other: 1\n",
            "Success Rate: 80.0%\n",
            "\n",
            " Detailed Results:\n",
            "✓ op_0: success (attempts: 1)\n",
            "✓ op_1: success (attempts: 1)\n",
            "✗ op_2: permission_denied (attempts: 1)\n",
            "   Error: Permission denied: [WinError 32] The process cannot access the file because it is being used by another process: 'locked_files_test\\\\document_2.txt'\n",
            "✓ op_3: success (attempts: 1)\n",
            "✓ op_4: success (attempts: 1)\n",
            "\n",
            " Final file listing:\n",
            "  - document_2.txt\n",
            "  - renamed_document_0.txt\n",
            "  - renamed_document_1.txt\n",
            "  - renamed_document_2.txt\n",
            "  - renamed_document_3.txt\n",
            "  - renamed_document_4.txt\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import logging\n",
        "import threading\n",
        "from enum import Enum\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Callable, List\n",
        "from queue import Queue, Empty\n",
        "from threading import Thread, Lock\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class FileOperationStatus(Enum):\n",
        "    \"\"\"Status of file operation attempts.\"\"\"\n",
        "    SUCCESS = \"success\"\n",
        "    LOCKED = \"locked\"\n",
        "    NOT_FOUND = \"not_found\"\n",
        "    PERMISSION_DENIED = \"permission_denied\"\n",
        "    FAILED = \"failed\"\n",
        "    QUEUED = \"queued\"\n",
        "    RETRYING = \"retrying\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FileOperation:\n",
        "    \"\"\"Represents a file operation to be performed.\"\"\"\n",
        "    operation_id: str\n",
        "    source_path: str\n",
        "    target_path: str\n",
        "    operation_type: str  # 'rename', 'move', 'copy'\n",
        "    max_retries: int = 3\n",
        "    retry_delay: float = 2.0\n",
        "    current_attempt: int = 0\n",
        "    status: FileOperationStatus = FileOperationStatus.QUEUED\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "\n",
        "class LockedFileHandler:\n",
        "    \"\"\"\n",
        "    Handle file operations with retry logic and queuing for locked files.\n",
        "    \n",
        "    Strategies implemented:\n",
        "    1. Retry with exponential backoff\n",
        "    2. Queue-based processing\n",
        "    3. File lock detection\n",
        "    4. Graceful degradation\n",
        "    5. Operation logging and monitoring\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_workers: int = 2, max_queue_size: int = 100):\n",
        "        self.operation_queue = Queue(maxsize=max_queue_size)\n",
        "        self.results = []\n",
        "        self.results_lock = Lock()\n",
        "        self.max_workers = max_workers\n",
        "        self.workers = []\n",
        "        self.running = False\n",
        "    \n",
        "    def is_file_locked(self, filepath: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if file is locked by another process.\n",
        "        \n",
        "        Strategy: Try to open file in exclusive mode.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Try to open with exclusive access\n",
        "            with open(filepath, 'a') as f:\n",
        "                # On Windows, try to get exclusive lock\n",
        "                if os.name == 'nt':\n",
        "                    import msvcrt\n",
        "                    try:\n",
        "                        msvcrt.locking(f.fileno(), msvcrt.LK_NBLCK, 1)\n",
        "                        msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)\n",
        "                        return False\n",
        "                    except IOError:\n",
        "                        return True\n",
        "                else:\n",
        "                    # On Unix, try fcntl lock\n",
        "                    import fcntl\n",
        "                    try:\n",
        "                        fcntl.flock(f.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
        "                        fcntl.flock(f.fileno(), fcntl.LOCK_UN)\n",
        "                        return False\n",
        "                    except IOError:\n",
        "                        return True\n",
        "        except PermissionError:\n",
        "            return True\n",
        "        except FileNotFoundError:\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not check lock status for {filepath}: {e}\")\n",
        "            return True\n",
        "    \n",
        "    def wait_for_file_unlock(self, filepath: str, timeout: float = 30.0, check_interval: float = 1.0) -> bool:\n",
        "        \"\"\"\n",
        "        Wait for file to become unlocked with timeout.\n",
        "        \n",
        "        Args:\n",
        "            filepath: Path to file\n",
        "            timeout: Maximum time to wait in seconds\n",
        "            check_interval: Time between lock checks\n",
        "        \n",
        "        Returns:\n",
        "            True if file became unlocked, False if timeout\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        while time.time() - start_time < timeout:\n",
        "            if not self.is_file_locked(filepath):\n",
        "                logger.info(f\"File {filepath} is now unlocked\")\n",
        "                return True\n",
        "            \n",
        "            logger.debug(f\"File {filepath} still locked, waiting...\")\n",
        "            time.sleep(check_interval)\n",
        "        \n",
        "        logger.warning(f\"Timeout waiting for {filepath} to unlock after {timeout}s\")\n",
        "        return False\n",
        "    \n",
        "    def perform_file_operation(self, operation: FileOperation) -> FileOperationStatus:\n",
        "        \"\"\"\n",
        "        Perform a file operation with lock handling.\n",
        "        \n",
        "        Returns:\n",
        "            Status of the operation\n",
        "        \"\"\"\n",
        "        source = operation.source_path\n",
        "        target = operation.target_path\n",
        "        op_type = operation.operation_type\n",
        "        \n",
        "        try:\n",
        "            # Check if source file exists\n",
        "            if not os.path.exists(source):\n",
        "                operation.error_message = f\"Source file not found: {source}\"\n",
        "                return FileOperationStatus.NOT_FOUND\n",
        "            \n",
        "            # Check if source is locked\n",
        "            if self.is_file_locked(source):\n",
        "                logger.warning(f\"File {source} is locked, waiting...\")\n",
        "                \n",
        "                # Wait for unlock with timeout\n",
        "                if self.wait_for_file_unlock(source, timeout=10.0):\n",
        "                    logger.info(f\"File {source} unlocked, proceeding with operation\")\n",
        "                else:\n",
        "                    operation.error_message = f\"File remains locked: {source}\"\n",
        "                    return FileOperationStatus.LOCKED\n",
        "            \n",
        "            # Check if target already exists and is locked\n",
        "            if os.path.exists(target) and self.is_file_locked(target):\n",
        "                operation.error_message = f\"Target file is locked: {target}\"\n",
        "                return FileOperationStatus.LOCKED\n",
        "            \n",
        "            # Perform the operation\n",
        "            if op_type == 'rename' or op_type == 'move':\n",
        "                shutil.move(source, target)\n",
        "                logger.info(f\"Successfully moved {source} -> {target}\")\n",
        "            elif op_type == 'copy':\n",
        "                shutil.copy2(source, target)\n",
        "                logger.info(f\"Successfully copied {source} -> {target}\")\n",
        "            else:\n",
        "                operation.error_message = f\"Unknown operation type: {op_type}\"\n",
        "                return FileOperationStatus.FAILED\n",
        "            \n",
        "            return FileOperationStatus.SUCCESS\n",
        "            \n",
        "        except PermissionError as e:\n",
        "            operation.error_message = f\"Permission denied: {e}\"\n",
        "            return FileOperationStatus.PERMISSION_DENIED\n",
        "        except Exception as e:\n",
        "            operation.error_message = f\"Operation failed: {e}\"\n",
        "            return FileOperationStatus.FAILED\n",
        "    \n",
        "    def process_operation_with_retry(self, operation: FileOperation) -> FileOperation:\n",
        "        \"\"\"\n",
        "        Process operation with retry logic and exponential backoff.\n",
        "        \"\"\"\n",
        "        operation.status = FileOperationStatus.RETRYING\n",
        "        \n",
        "        while operation.current_attempt < operation.max_retries:\n",
        "            operation.current_attempt += 1\n",
        "            \n",
        "            logger.info(f\"Attempt {operation.current_attempt}/{operation.max_retries} for {operation.operation_id}\")\n",
        "            \n",
        "            status = self.perform_file_operation(operation)\n",
        "            operation.status = status\n",
        "            \n",
        "            if status == FileOperationStatus.SUCCESS:\n",
        "                logger.info(f\"✓ Operation {operation.operation_id} completed successfully\")\n",
        "                return operation\n",
        "            \n",
        "            # If locked or other retryable error, wait and retry\n",
        "            if status in [FileOperationStatus.LOCKED, FileOperationStatus.FAILED]:\n",
        "                if operation.current_attempt < operation.max_retries:\n",
        "                    # Exponential backoff: 2s, 4s, 8s\n",
        "                    delay = operation.retry_delay * (2 ** (operation.current_attempt - 1))\n",
        "                    logger.info(f\"Retrying in {delay}s... ({status.value})\")\n",
        "                    time.sleep(delay)\n",
        "                else:\n",
        "                    logger.error(f\"✗ Operation {operation.operation_id} failed after {operation.max_retries} attempts\")\n",
        "            else:\n",
        "                # Non-retryable error (not found, permission denied)\n",
        "                logger.error(f\"✗ Operation {operation.operation_id} failed: {operation.error_message}\")\n",
        "                return operation\n",
        "        \n",
        "        return operation\n",
        "    \n",
        "    def worker_thread(self):\n",
        "        \"\"\"Worker thread that processes operations from queue.\"\"\"\n",
        "        logger.info(f\"Worker thread started (Thread-{threading.current_thread().ident})\")\n",
        "        \n",
        "        while self.running:\n",
        "            try:\n",
        "                # Get operation from queue with timeout\n",
        "                operation = self.operation_queue.get(timeout=1.0)\n",
        "                \n",
        "                logger.info(f\"Processing operation: {operation.operation_id}\")\n",
        "                \n",
        "                # Process with retry logic\n",
        "                completed_operation = self.process_operation_with_retry(operation)\n",
        "                \n",
        "                # Store result\n",
        "                with self.results_lock:\n",
        "                    self.results.append(completed_operation)\n",
        "                \n",
        "                # Mark task as done\n",
        "                self.operation_queue.task_done()\n",
        "                \n",
        "            except Empty:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Worker thread error: {e}\")\n",
        "        \n",
        "        logger.info(f\"Worker thread stopped (Thread-{threading.current_thread().ident})\")\n",
        "    \n",
        "    def start_workers(self):\n",
        "        \"\"\"Start worker threads for processing operations.\"\"\"\n",
        "        if self.running:\n",
        "            logger.warning(\"Workers already running\")\n",
        "            return\n",
        "        \n",
        "        self.running = True\n",
        "        \n",
        "        for i in range(self.max_workers):\n",
        "            worker = Thread(target=self.worker_thread, daemon=True, name=f\"FileWorker-{i}\")\n",
        "            worker.start()\n",
        "            self.workers.append(worker)\n",
        "        \n",
        "        logger.info(f\"Started {self.max_workers} worker threads\")\n",
        "    \n",
        "    def stop_workers(self, wait: bool = True):\n",
        "        \"\"\"Stop worker threads.\"\"\"\n",
        "        self.running = False\n",
        "        \n",
        "        if wait:\n",
        "            for worker in self.workers:\n",
        "                worker.join(timeout=5.0)\n",
        "        \n",
        "        logger.info(\"All workers stopped\")\n",
        "    \n",
        "    def queue_operation(self, operation: FileOperation) -> bool:\n",
        "        \"\"\"\n",
        "        Add operation to queue.\n",
        "        \n",
        "        Returns:\n",
        "            True if queued successfully, False if queue is full\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.operation_queue.put(operation, block=False)\n",
        "            logger.info(f\"Queued operation: {operation.operation_id}\")\n",
        "            return True\n",
        "        except:\n",
        "            logger.error(f\"Queue is full, cannot add operation: {operation.operation_id}\")\n",
        "            return False\n",
        "    \n",
        "    def wait_for_completion(self, timeout: Optional[float] = None):\n",
        "        \"\"\"Wait for all queued operations to complete.\"\"\"\n",
        "        logger.info(\"Waiting for all operations to complete...\")\n",
        "        \n",
        "        if timeout:\n",
        "            self.operation_queue.join()\n",
        "        else:\n",
        "            # Wait with timeout\n",
        "            start_time = time.time()\n",
        "            while not self.operation_queue.empty():\n",
        "                if timeout and (time.time() - start_time) > timeout:\n",
        "                    logger.warning(f\"Timeout waiting for queue completion\")\n",
        "                    break\n",
        "                time.sleep(0.5)\n",
        "        \n",
        "        logger.info(\"All operations completed\")\n",
        "    \n",
        "    def get_results_summary(self) -> dict:\n",
        "        \"\"\"Get summary of operation results.\"\"\"\n",
        "        with self.results_lock:\n",
        "            total = len(self.results)\n",
        "            success = sum(1 for r in self.results if r.status == FileOperationStatus.SUCCESS)\n",
        "            locked = sum(1 for r in self.results if r.status == FileOperationStatus.LOCKED)\n",
        "            failed = sum(1 for r in self.results if r.status == FileOperationStatus.FAILED)\n",
        "            other = total - success - locked - failed\n",
        "            \n",
        "            return {\n",
        "                'total_operations': total,\n",
        "                'successful': success,\n",
        "                'locked': locked,\n",
        "                'failed': failed,\n",
        "                'other': other,\n",
        "                'success_rate': f\"{(success/total*100):.1f}%\" if total > 0 else \"0%\"\n",
        "            }\n",
        "\n",
        "\n",
        "# ===================== DEMONSTRATION =====================\n",
        "\n",
        "print(\"=== LOCKED FILE HANDLER DEMONSTRATION ===\\n\")\n",
        "\n",
        "# Create test directory and files\n",
        "test_dir = Path(\"locked_files_test\")\n",
        "test_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create some test files\n",
        "test_files = []\n",
        "for i in range(5):\n",
        "    filepath = test_dir / f\"document_{i}.txt\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(f\"Document {i} content\\n\" * 10)\n",
        "    test_files.append(filepath)\n",
        "\n",
        "print(f\"Created {len(test_files)} test files\\n\")\n",
        "\n",
        "# Initialize handler\n",
        "handler = LockedFileHandler(max_workers=2)\n",
        "\n",
        "# Start worker threads\n",
        "handler.start_workers()\n",
        "\n",
        "# Queue some operations\n",
        "print(\"Queueing file operations...\\n\")\n",
        "\n",
        "operations = [\n",
        "    FileOperation(\n",
        "        operation_id=f\"op_{i}\",\n",
        "        source_path=str(test_files[i]),\n",
        "        target_path=str(test_dir / f\"renamed_document_{i}.txt\"),\n",
        "        operation_type='rename',\n",
        "        max_retries=3,\n",
        "        retry_delay=1.0\n",
        "    )\n",
        "    for i in range(5)\n",
        "]\n",
        "\n",
        "# Queue all operations\n",
        "for op in operations:\n",
        "    handler.queue_operation(op)\n",
        "\n",
        "# Simulate a locked file scenario\n",
        "print(\"\\n Simulating locked file scenario...\")\n",
        "print(\"Opening document_2.txt to lock it...\")\n",
        "\n",
        "# Open a file to lock it (in real scenario, Excel/Word/etc would lock it)\n",
        "locked_file = open(test_files[2], 'r')\n",
        "print(f\"✓ File {test_files[2].name} is now locked\\n\")\n",
        "\n",
        "# Wait a bit, then unlock\n",
        "time.sleep(5)\n",
        "print(\"\\n Unlocking file...\")\n",
        "locked_file.close()\n",
        "print(\"✓ File unlocked\\n\")\n",
        "\n",
        "# Wait for all operations to complete\n",
        "handler.wait_for_completion()\n",
        "\n",
        "# Stop workers\n",
        "handler.stop_workers()\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = handler.get_results_summary()\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "print(\"\\n Detailed Results:\")\n",
        "with handler.results_lock:\n",
        "    for result in handler.results:\n",
        "        status_emoji = \"✓\" if result.status == FileOperationStatus.SUCCESS else \"✗\"\n",
        "        print(f\"{status_emoji} {result.operation_id}: {result.status.value} (attempts: {result.current_attempt})\")\n",
        "        if result.error_message:\n",
        "            print(f\"   Error: {result.error_message}\")\n",
        "\n",
        "print(\"\\n Final file listing:\")\n",
        "for item in sorted(test_dir.iterdir()):\n",
        "    print(f\"  - {item.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e499d46",
      "metadata": {},
      "source": [
        "### **Explanation: Strategies for Locked File Handling**\n",
        "\n",
        "**1. File Lock Detection:**\n",
        "- **Windows**: Use `msvcrt.locking()` to check for exclusive file locks\n",
        "- **Unix/Linux**: Use `fcntl.flock()` for file locking detection\n",
        "- **Cross-platform**: Try opening file in append mode with exclusive access\n",
        "- **Fallback**: Catch `PermissionError` and `IOError` exceptions\n",
        "\n",
        "**2. Retry with Exponential Backoff:**\n",
        "- **First attempt**: Immediate execution\n",
        "- **Retry delays**: 2s → 4s → 8s (exponential growth: `2^(attempt-1)`)\n",
        "- **Max retries**: Configurable (default: 3 attempts)\n",
        "- **Benefits**: Gives other processes time to release locks without aggressive polling\n",
        "\n",
        "**3. Queue-Based Processing:**\n",
        "- **`Queue` class**: Thread-safe FIFO queue for operations\n",
        "- **Worker threads**: Multiple threads process operations concurrently\n",
        "- **Graceful degradation**: System continues working even if some files are locked\n",
        "- **Non-blocking**: Main program can continue while operations are processed\n",
        "\n",
        "**4. Operation States:**\n",
        "- **QUEUED**: Operation added to queue, waiting for worker\n",
        "- **RETRYING**: Currently attempting operation with retries\n",
        "- **SUCCESS**: Operation completed successfully\n",
        "- **LOCKED**: File remains locked after all retries\n",
        "- **FAILED**: Other error occurred (permissions, disk space, etc.)\n",
        "- **NOT_FOUND**: Source file doesn't exist\n",
        "\n",
        "**5. Thread Safety Patterns:**\n",
        "- **`Lock()`**: Protects shared results list from race conditions\n",
        "- **`daemon=True`**: Worker threads don't prevent program exit\n",
        "- **`queue.task_done()`**: Signals completion for `queue.join()` synchronization\n",
        "- **`timeout` parameters**: Prevent indefinite blocking\n",
        "\n",
        "**6. Production Enhancements:**\n",
        "```python\n",
        "# Add callback for completion notifications\n",
        "def on_operation_complete(operation: FileOperation):\n",
        "    if operation.status == FileOperationStatus.SUCCESS:\n",
        "        send_notification(f\"✓ {operation.operation_id} completed\")\n",
        "    else:\n",
        "        send_alert(f\"✗ {operation.operation_id} failed: {operation.error_message}\")\n",
        "\n",
        "# Add operation priority queue\n",
        "from queue import PriorityQueue\n",
        "priority_queue = PriorityQueue()\n",
        "priority_queue.put((1, high_priority_op))  # Lower number = higher priority\n",
        "priority_queue.put((5, low_priority_op))\n",
        "\n",
        "# Add monitoring and metrics\n",
        "metrics = {\n",
        "    'total_operations': 0,\n",
        "    'total_retries': 0,\n",
        "    'avg_completion_time': 0,\n",
        "    'lock_conflicts': 0\n",
        "}\n",
        "\n",
        "# Add dead letter queue for permanently failed operations\n",
        "failed_operations_queue = []\n",
        "\n",
        "# Add automatic cleanup of old operations\n",
        "def cleanup_old_operations(age_hours: int = 24):\n",
        "    cutoff = datetime.now() - timedelta(hours=age_hours)\n",
        "    # Remove operations older than cutoff\n",
        "```\n",
        "\n",
        "**Alternative Strategies:**\n",
        "\n",
        "**A. Shadow Copy / Snapshot Approach:**\n",
        "```python\n",
        "# On Windows, use Volume Shadow Copy\n",
        "import win32com.client\n",
        "\n",
        "def create_shadow_copy(volume: str) -> str:\n",
        "    \"\"\"Create VSS snapshot and return shadow copy path.\"\"\"\n",
        "    # Useful for backing up files that are locked\n",
        "    pass\n",
        "\n",
        "# Access locked files via shadow copy\n",
        "shadow_path = create_shadow_copy(\"C:\\\\\")\n",
        "copy_from_shadow(shadow_path + \"locked_file.xlsx\")\n",
        "```\n",
        "\n",
        "**B. File System Watcher:**\n",
        "```python\n",
        "from watchdog.observers import Observer\n",
        "from watchdog.events import FileSystemEventHandler\n",
        "\n",
        "class UnlockHandler(FileSystemEventHandler):\n",
        "    def on_modified(self, event):\n",
        "        # When file is closed/unlocked, retry operation\n",
        "        if not is_file_locked(event.src_path):\n",
        "            retry_pending_operation(event.src_path)\n",
        "```\n",
        "\n",
        "**C. Scheduled Retry (Cron-like):**\n",
        "```python\n",
        "# Instead of immediate retry, schedule for later\n",
        "scheduler.schedule_retry(\n",
        "    operation=failed_op,\n",
        "    retry_after=timedelta(minutes=5),\n",
        "    max_retries=10\n",
        ")\n",
        "```\n",
        "\n",
        "**D. User Notification Pattern:**\n",
        "```python\n",
        "# Notify user to close file\n",
        "def request_file_closure(filepath: str):\n",
        "    # Send email/Slack message to file owner\n",
        "    owner = get_file_owner(filepath)\n",
        "    send_message(\n",
        "        to=owner,\n",
        "        subject=f\"Please close {filepath}\",\n",
        "        body=\"Automated process needs access to this file.\"\n",
        "    )\n",
        "```\n",
        "\n",
        "**DevOps Use Cases:**\n",
        "- **Log rotation**: Rotate logs that may be locked by running services\n",
        "- **Backup operations**: Backup files that users have open\n",
        "- **Configuration updates**: Update config files in use by applications\n",
        "- **Deployment scripts**: Replace DLLs/executables that are loaded\n",
        "- **Report generation**: Process Excel files that analysts have open\n",
        "- **Database file operations**: Handle locked SQLite databases\n",
        "\n",
        "**Best Practices:**\n",
        "1. Always implement timeout limits to prevent infinite waits\n",
        "2. Log all retry attempts for debugging and monitoring\n",
        "3. Provide clear error messages indicating why operations failed\n",
        "4. Use exponential backoff to avoid overwhelming the system\n",
        "5. Implement circuit breaker pattern for repeatedly failing operations\n",
        "6. Monitor queue depth to detect systemic issues\n",
        "7. Set maximum queue size to prevent memory exhaustion\n",
        "8. Gracefully handle worker thread failures\n",
        "9. Provide manual retry capability for operators\n",
        "10. Keep audit trail of all file operations for compliance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de5028d",
      "metadata": {
        "id": "7de5028d"
      },
      "source": [
        "## Section 4: Data Processing and Excel Reporting\n",
        "\n",
        "### Theory: Data Pipeline Development\n",
        "\n",
        "Data processing is central to DevOps observability:\n",
        "- **ETL Operations** - Extract, Transform, Load data from multiple sources\n",
        "- **Aggregation** - Summarizing metrics across time periods and services\n",
        "- **Reporting** - Creating consumable reports for stakeholders\n",
        "- **Data Validation** - Ensuring data quality and consistency\n",
        "\n",
        "### Essential Data Processing Methods:\n",
        "- **`pandas.read_csv()`** - Read CSV files into DataFrames\n",
        "- **`DataFrame.groupby()`** - Group data for aggregation operations\n",
        "- **`DataFrame.agg()`** - Apply aggregation functions (sum, mean, count)\n",
        "- **`ExcelWriter`** - Create multi-sheet Excel workbooks\n",
        "- **`to_excel()`** - Write DataFrames to Excel sheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e7855f",
      "metadata": {
        "id": "c5e7855f",
        "outputId": "97857bd7-be9b-42e0-93c4-ad2e8f4ee179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting openpyxl\n",
            "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "\n",
            "   -------------------- ------------------- 1/2 [openpyxl]\n",
            "   -------------------- ------------------- 1/2 [openpyxl]\n",
            "   -------------------- ------------------- 1/2 [openpyxl]\n",
            "   -------------------- ------------------- 1/2 [openpyxl]\n",
            "   -------------------- ------------------- 1/2 [openpyxl]\n",
            "   -------------------- ------------------- 1/2 [openpyxl]\n",
            "   ---------------------------------------- 2/2 [openpyxl]\n",
            "\n",
            "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01e9fc1",
      "metadata": {
        "id": "f01e9fc1",
        "outputId": "78810f2d-27eb-4fbe-821d-c214935e9be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CSV TO EXCEL REPORT BUILDER ===\n",
            "Created transactions_20251026.csv with 100 transactions\n",
            "Created transactions_20251025.csv with 100 transactions\n",
            "Created transactions_20251024.csv with 100 transactions\n",
            "Loaded transactions_20251024.csv: 100 transactions\n",
            "Loaded transactions_20251025.csv: 100 transactions\n",
            "Loaded transactions_20251026.csv: 100 transactions\n",
            "\n",
            "Total transactions loaded: 300\n",
            "\n",
            " Daily Summary (first 3 rows):\n",
            "         date  total_transactions  total_amount  avg_amount  unique_users  \\\n",
            "0  2025-10-24                  33       8455.02      256.21            24   \n",
            "1  2025-10-25                 108      26036.21      241.08            41   \n",
            "2  2025-10-26                  97      25493.74      262.82            46   \n",
            "\n",
            "   unique_services  \n",
            "0                4  \n",
            "1                4  \n",
            "2                4  \n",
            "\n",
            " Service Summary:\n",
            "                service  total_transactions  total_amount  avg_amount  \\\n",
            "2       payment-service                  79      20750.73      262.67   \n",
            "3          user-service                  80      19145.80      239.32   \n",
            "1  notification-service                  71      18121.74      255.24   \n",
            "0          auth-service                  70      17572.03      251.03   \n",
            "\n",
            "   min_amount  max_amount  unique_users  \n",
            "2        7.79      498.90            40  \n",
            "3       18.89      499.17            39  \n",
            "1       17.74      499.66            36  \n",
            "0        5.92      498.28            42  \n",
            "\n",
            " Excel report created: transaction_report.xlsx\n",
            "\n",
            "=== REPORT SUMMARY ===\n",
            "Daily Summary: 4 days\n",
            "Service Summary: 4 services\n",
            "Raw Data: 300 total transactions\n",
            "Total Amount: $75,590.30\n",
            "Average Transaction: $251.97\n",
            "\n",
            "Top 3 Services by Volume:\n",
            "  payment-service: $20,750.73 (79 transactions)\n",
            "  user-service: $19,145.80 (80 transactions)\n",
            "  notification-service: $18,121.74 (71 transactions)\n"
          ]
        }
      ],
      "source": [
        "# Example 5: CSV to Excel Report Builder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "class ReportBuilder:\n",
        "    \"\"\"Build comprehensive Excel reports from CSV transaction data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_folder: str):\n",
        "        self.input_folder = input_folder\n",
        "        self.raw_data = None\n",
        "        self.daily_summary = None\n",
        "        self.service_summary = None\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Create sample CSV files for testing.\"\"\"\n",
        "        os.makedirs(self.input_folder, exist_ok=True)\n",
        "\n",
        "        # Generate sample transaction data\n",
        "        services = ['auth-service', 'user-service', 'payment-service', 'notification-service']\n",
        "        users = [f'user_{i:03d}' for i in range(1, 51)]  # 50 users\n",
        "\n",
        "        # Create 3 CSV files representing different days\n",
        "        for day_offset in range(3):\n",
        "            date = datetime.now() - timedelta(days=day_offset)\n",
        "            filename = f\"transactions_{date.strftime('%Y%m%d')}.csv\"\n",
        "            filepath = os.path.join(self.input_folder, filename)\n",
        "\n",
        "            # Generate random transactions for this day\n",
        "            transactions = []\n",
        "            for _ in range(100):  # 100 transactions per day\n",
        "                timestamp = date + timedelta(\n",
        "                    hours=np.random.randint(0, 24),\n",
        "                    minutes=np.random.randint(0, 60)\n",
        "                )\n",
        "\n",
        "                transaction = {\n",
        "                    'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'user_id': np.random.choice(users),\n",
        "                    'service': np.random.choice(services),\n",
        "                    'amount': round(np.random.uniform(5.0, 500.0), 2)\n",
        "                }\n",
        "                transactions.append(transaction)\n",
        "\n",
        "            # Write to CSV\n",
        "            df = pd.DataFrame(transactions)\n",
        "            df.to_csv(filepath, index=False)\n",
        "            print(f\"Created {filename} with {len(transactions)} transactions\")\n",
        "\n",
        "    def load_csv_files(self):\n",
        "        \"\"\"Load and merge all CSV files from input folder.\"\"\"\n",
        "        csv_files = [f for f in os.listdir(self.input_folder) if f.endswith('.csv')]\n",
        "\n",
        "        if not csv_files:\n",
        "            raise ValueError(f\"No CSV files found in {self.input_folder}\")\n",
        "\n",
        "        dataframes = []\n",
        "\n",
        "        for csv_file in csv_files:\n",
        "            filepath = os.path.join(self.input_folder, csv_file)\n",
        "            try:\n",
        "                df = pd.read_csv(filepath)\n",
        "\n",
        "                # Validate required columns\n",
        "                required_columns = ['timestamp', 'user_id', 'service', 'amount']\n",
        "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "                if missing_columns:\n",
        "                    print(f\"Warning: {csv_file} missing columns: {missing_columns}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert timestamp to datetime\n",
        "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "                df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "                dataframes.append(df)\n",
        "                print(f\"Loaded {csv_file}: {len(df)} transactions\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {csv_file}: {e}\")\n",
        "\n",
        "        if not dataframes:\n",
        "            raise ValueError(\"No valid CSV files could be loaded\")\n",
        "\n",
        "        # Merge all data\n",
        "        self.raw_data = pd.concat(dataframes, ignore_index=True)\n",
        "        print(f\"\\nTotal transactions loaded: {len(self.raw_data)}\")\n",
        "\n",
        "        return self.raw_data\n",
        "\n",
        "    def generate_daily_summary(self):\n",
        "        \"\"\"Generate daily aggregated summary.\"\"\"\n",
        "        if self.raw_data is None:\n",
        "            raise ValueError(\"No data loaded. Call load_csv_files() first.\")\n",
        "\n",
        "        self.daily_summary = self.raw_data.groupby('date').agg({\n",
        "            'amount': ['count', 'sum', 'mean'],\n",
        "            'user_id': 'nunique',\n",
        "            'service': 'nunique'\n",
        "        }).round(2)\n",
        "\n",
        "        # Flatten column names\n",
        "        self.daily_summary.columns = [\n",
        "            'total_transactions', 'total_amount', 'avg_amount',\n",
        "            'unique_users', 'unique_services'\n",
        "        ]\n",
        "\n",
        "        # Reset index to make date a column\n",
        "        self.daily_summary.reset_index(inplace=True)\n",
        "\n",
        "        return self.daily_summary\n",
        "\n",
        "    def generate_service_summary(self):\n",
        "        \"\"\"Generate service-level aggregated summary.\"\"\"\n",
        "        if self.raw_data is None:\n",
        "            raise ValueError(\"No data loaded. Call load_csv_files() first.\")\n",
        "\n",
        "        self.service_summary = self.raw_data.groupby('service').agg({\n",
        "            'amount': ['count', 'sum', 'mean', 'min', 'max'],\n",
        "            'user_id': 'nunique'\n",
        "        }).round(2)\n",
        "\n",
        "        # Flatten column names\n",
        "        self.service_summary.columns = [\n",
        "            'total_transactions', 'total_amount', 'avg_amount',\n",
        "            'min_amount', 'max_amount', 'unique_users'\n",
        "        ]\n",
        "\n",
        "        # Reset index to make service a column\n",
        "        self.service_summary.reset_index(inplace=True)\n",
        "\n",
        "        # Sort by total amount descending\n",
        "        self.service_summary = self.service_summary.sort_values('total_amount', ascending=False)\n",
        "\n",
        "        return self.service_summary\n",
        "\n",
        "    def create_excel_report(self, output_filename: str):\n",
        "        \"\"\"Create comprehensive Excel report with multiple sheets.\"\"\"\n",
        "        with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
        "            # Write daily summary\n",
        "            self.daily_summary.to_excel(writer, sheet_name='DailySummary', index=False)\n",
        "\n",
        "            # Write service summary\n",
        "            self.service_summary.to_excel(writer, sheet_name='ServiceSummary', index=False)\n",
        "\n",
        "            # Write raw merged data (limited to first 1000 rows for performance)\n",
        "            raw_limited = self.raw_data.head(1000).copy()\n",
        "            raw_limited.to_excel(writer, sheet_name='RawMerged', index=False)\n",
        "\n",
        "            # Apply formatting\n",
        "            self._format_excel_sheets(writer)\n",
        "\n",
        "        print(f\"\\n Excel report created: {output_filename}\")\n",
        "\n",
        "        # Print summary statistics\n",
        "        self._print_summary_stats()\n",
        "\n",
        "    def _format_excel_sheets(self, writer):\n",
        "        \"\"\"Apply formatting to Excel sheets.\"\"\"\n",
        "        # Format Daily Summary sheet\n",
        "        ws_daily = writer.sheets['DailySummary']\n",
        "\n",
        "        # Header formatting\n",
        "        header_font = Font(bold=True, color=\"FFFFFF\")\n",
        "        header_fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
        "\n",
        "        for cell in ws_daily[1]:  # First row (headers)\n",
        "            cell.font = header_font\n",
        "            cell.fill = header_fill\n",
        "            cell.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "        # Auto-adjust column widths\n",
        "        for column in ws_daily.columns:\n",
        "            max_length = 0\n",
        "            column_letter = column[0].column_letter\n",
        "            for cell in column:\n",
        "                try:\n",
        "                    if len(str(cell.value)) > max_length:\n",
        "                        max_length = len(str(cell.value))\n",
        "                except:\n",
        "                    pass\n",
        "            adjusted_width = min(max_length + 2, 20)\n",
        "            ws_daily.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "    def _print_summary_stats(self):\n",
        "        \"\"\"Print summary statistics for validation.\"\"\"\n",
        "        print(\"\\n=== REPORT SUMMARY ===\")\n",
        "        print(f\"Daily Summary: {len(self.daily_summary)} days\")\n",
        "        print(f\"Service Summary: {len(self.service_summary)} services\")\n",
        "        print(f\"Raw Data: {len(self.raw_data)} total transactions\")\n",
        "\n",
        "        total_amount = self.raw_data['amount'].sum()\n",
        "        avg_amount = self.raw_data['amount'].mean()\n",
        "        print(f\"Total Amount: ${total_amount:,.2f}\")\n",
        "        print(f\"Average Transaction: ${avg_amount:.2f}\")\n",
        "\n",
        "        print(\"\\nTop 3 Services by Volume:\")\n",
        "        for idx, row in self.service_summary.head(3).iterrows():\n",
        "            print(f\"  {row['service']}: ${row['total_amount']:,.2f} ({row['total_transactions']} transactions)\")\n",
        "\n",
        "# Test the report builder\n",
        "print(\"=== CSV TO EXCEL REPORT BUILDER ===\")\n",
        "\n",
        "# Create report builder and sample data\n",
        "builder = ReportBuilder('transaction_data')\n",
        "builder.create_sample_data()\n",
        "\n",
        "# Load and process data\n",
        "raw_data = builder.load_csv_files()\n",
        "daily_summary = builder.generate_daily_summary()\n",
        "service_summary = builder.generate_service_summary()\n",
        "\n",
        "print(\"\\n Daily Summary (first 3 rows):\")\n",
        "print(daily_summary.head(3))\n",
        "\n",
        "print(\"\\n Service Summary:\")\n",
        "print(service_summary)\n",
        "\n",
        "# Create Excel report\n",
        "builder.create_excel_report('transaction_report.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d32355",
      "metadata": {
        "id": "12d32355"
      },
      "source": [
        "### Key Data Processing Methods:\n",
        "\n",
        "**Pandas DataFrame Operations:**\n",
        "- **`pd.read_csv(filepath)`** - Read CSV files into DataFrame objects\n",
        "- **`pd.concat(dataframes)`** - Concatenate multiple DataFrames vertically\n",
        "- **`df.groupby(column)`** - Group DataFrame rows by column values\n",
        "- **`df.agg(functions)`** - Apply aggregation functions to grouped data\n",
        "- **`pd.to_datetime(series)`** - Convert strings to datetime objects\n",
        "\n",
        "**Data Aggregation Functions:**\n",
        "- **`'count'`** - Count non-null values in each group\n",
        "- **`'sum'`** - Sum numeric values in each group\n",
        "- **`'mean'`** - Calculate average values in each group\n",
        "- **`'nunique'`** - Count unique values in each group\n",
        "- **`'min'/'max'`** - Find minimum/maximum values\n",
        "\n",
        "**Excel Integration:**\n",
        "- **`pd.ExcelWriter(filename, engine='openpyxl')`** - Create Excel file writer\n",
        "- **`df.to_excel(writer, sheet_name)`** - Write DataFrame to Excel sheet\n",
        "- **`openpyxl.styles`** - Apply formatting (Font, PatternFill, Alignment)\n",
        "- **Context managers** - `with writer:` ensures proper file closure\n",
        "\n",
        "**Data Validation Patterns:**\n",
        "- **Column existence checking** - Validate required columns are present\n",
        "- **Exception handling** - Graceful handling of file format errors\n",
        "- **Data type conversion** - Convert strings to appropriate types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bdbe945",
      "metadata": {
        "id": "3bdbe945"
      },
      "source": [
        "## Section 5: Concurrent Programming and HTTP Operations\n",
        "\n",
        "### Theory: Asynchronous Operations in DevOps\n",
        "\n",
        "Concurrent programming is essential for:\n",
        "- **Health monitoring** - Checking multiple services simultaneously\n",
        "- **API testing** - Parallel validation of endpoints\n",
        "- **Data collection** - Gathering metrics from multiple sources\n",
        "- **Performance optimization** - Reducing total execution time\n",
        "\n",
        "### Essential Async and HTTP Methods:\n",
        "- **`asyncio.run()`** - Run async functions from synchronous code\n",
        "- **`aiohttp.ClientSession()`** - HTTP client for async requests\n",
        "- **`asyncio.gather()`** - Run multiple async functions concurrently\n",
        "- **`asyncio.sleep()`** - Non-blocking delay for retry logic\n",
        "- **Exponential backoff** - Progressive retry delays for resilience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feef8d6c",
      "metadata": {
        "id": "feef8d6c",
        "outputId": "908b325b-79d9-49c3-c62d-1b5aa5f71452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiohttp in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (3.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hmeln\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hmeln\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\hmeln\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\hmeln\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp) (4.11.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~lask (c:\\Users\\hmeln\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lask (c:\\Users\\hmeln\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lask (c:\\Users\\hmeln\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install aiohttp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89011916",
      "metadata": {
        "id": "89011916",
        "outputId": "0d771d44-5f4a-457c-a87c-90f872b39572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CONCURRENT REST HEALTH CHECKER ===\n",
            "Created sample endpoints configuration: endpoints.json\n",
            "Loaded 5 endpoints from endpoints.json\n",
            "\n",
            " Starting health checks for 5 endpoints...\n",
            " invalid-url: Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed], retrying...\n",
            "   invalid-url: Retry 1 after 1s delay\n",
            " invalid-url: Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed], retrying...\n",
            "   invalid-url: Retry 1 after 1s delay\n",
            " invalid-url: Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed], retrying...\n",
            "   invalid-url: Retry 2 after 2s delay\n",
            " invalid-url: Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed], retrying...\n",
            "   invalid-url: Retry 2 after 2s delay\n",
            " invalid-url: FAILED after 3 attempts - Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed]\n",
            " invalid-url: FAILED after 3 attempts - Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed]\n",
            " httpbin-delay: Timeout after 5s, retrying...\n",
            "   httpbin-delay: Retry 1 after 1s delay\n",
            " httpbin-get: Timeout after 5s, retrying...\n",
            "   httpbin-get: Retry 1 after 1s delay\n",
            " google: Timeout after 5s, retrying...\n",
            "   google: Retry 1 after 1s delay\n",
            " httpbin-status-200: Timeout after 5s, retrying...\n",
            "   httpbin-status-200: Retry 1 after 1s delay\n",
            " httpbin-delay: Timeout after 5s, retrying...\n",
            "   httpbin-delay: Retry 1 after 1s delay\n",
            " httpbin-get: Timeout after 5s, retrying...\n",
            "   httpbin-get: Retry 1 after 1s delay\n",
            " google: Timeout after 5s, retrying...\n",
            "   google: Retry 1 after 1s delay\n",
            " httpbin-status-200: Timeout after 5s, retrying...\n",
            "   httpbin-status-200: Retry 1 after 1s delay\n",
            " google: Timeout after 5s, retrying...\n",
            "   google: Retry 2 after 2s delay\n",
            " httpbin-delay: Timeout after 5s, retrying...\n",
            "   httpbin-delay: Retry 2 after 2s delay\n",
            " httpbin-status-200: Timeout after 5s, retrying...\n",
            "   httpbin-status-200: Retry 2 after 2s delay\n",
            " httpbin-get: Timeout after 5s, retrying...\n",
            "   httpbin-get: Retry 2 after 2s delay\n",
            " google: Timeout after 5s, retrying...\n",
            "   google: Retry 2 after 2s delay\n",
            " httpbin-delay: Timeout after 5s, retrying...\n",
            "   httpbin-delay: Retry 2 after 2s delay\n",
            " httpbin-status-200: Timeout after 5s, retrying...\n",
            "   httpbin-status-200: Retry 2 after 2s delay\n",
            " httpbin-get: Timeout after 5s, retrying...\n",
            "   httpbin-get: Retry 2 after 2s delay\n",
            " httpbin-delay: FAILED after 3 attempts - Timeout after 5s\n",
            " google: FAILED after 3 attempts - Timeout after 5s\n",
            " httpbin-get: FAILED after 3 attempts - Timeout after 5s\n",
            " httpbin-status-200: FAILED after 3 attempts - Timeout after 5s\n",
            "\n",
            "  Total execution time: 18.16s\n",
            " Results saved to CSV: health_check_results.csv\n",
            " Results saved to JSON: health_check_results.json\n",
            "\n",
            " HEALTH CHECK SUMMARY\n",
            "Total endpoints: 5\n",
            " Successful: 0\n",
            " Failed: 5\n",
            "\n",
            " Failed endpoints:\n",
            "  - httpbin-get: Timeout after 5s\n",
            "  - httpbin-status-200: Timeout after 5s\n",
            "  - httpbin-delay: Timeout after 5s\n",
            "  - google: Timeout after 5s\n",
            "  - invalid-url: Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed]\n",
            " httpbin-delay: FAILED after 3 attempts - Timeout after 5s\n",
            " google: FAILED after 3 attempts - Timeout after 5s\n",
            " httpbin-get: FAILED after 3 attempts - Timeout after 5s\n",
            " httpbin-status-200: FAILED after 3 attempts - Timeout after 5s\n",
            "\n",
            "  Total execution time: 18.16s\n",
            " Results saved to CSV: health_check_results.csv\n",
            " Results saved to JSON: health_check_results.json\n",
            "\n",
            " HEALTH CHECK SUMMARY\n",
            "Total endpoints: 5\n",
            " Successful: 0\n",
            " Failed: 5\n",
            "\n",
            " Failed endpoints:\n",
            "  - httpbin-get: Timeout after 5s\n",
            "  - httpbin-status-200: Timeout after 5s\n",
            "  - httpbin-delay: Timeout after 5s\n",
            "  - google: Timeout after 5s\n",
            "  - invalid-url: Connection error: Cannot connect to host this-domain-does-not-exist-12345.com:443 ssl:default [getaddrinfo failed]\n"
          ]
        }
      ],
      "source": [
        "# Example 6: Concurrent REST Health Checker\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "class HealthChecker:\n",
        "    \"\"\"Concurrent HTTP health checker with retry logic and exponential backoff.\"\"\"\n",
        "\n",
        "    def __init__(self, timeout: int = 10, max_retries: int = 3):\n",
        "        self.timeout = timeout\n",
        "        self.max_retries = max_retries\n",
        "        self.results = []\n",
        "\n",
        "    def create_sample_endpoints(self, filename: str):\n",
        "        \"\"\"Create sample endpoint configuration for testing.\"\"\"\n",
        "        endpoints = [\n",
        "            {\"name\": \"httpbin-get\", \"url\": \"https://httpbin.org/get\", \"expected_status\": 200},\n",
        "            {\"name\": \"httpbin-status-200\", \"url\": \"https://httpbin.org/status/200\", \"expected_status\": 200},\n",
        "            {\"name\": \"httpbin-delay\", \"url\": \"https://httpbin.org/delay/1\", \"expected_status\": 200},\n",
        "            {\"name\": \"google\", \"url\": \"https://www.google.com\", \"expected_status\": 200},\n",
        "            {\"name\": \"invalid-url\", \"url\": \"https://this-domain-does-not-exist-12345.com\", \"expected_status\": 200}\n",
        "        ]\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(endpoints, f, indent=2)\n",
        "\n",
        "        print(f\"Created sample endpoints configuration: {filename}\")\n",
        "        return endpoints\n",
        "\n",
        "    def load_endpoints(self, filename: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load endpoint configuration from JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                endpoints = json.load(f)\n",
        "\n",
        "            # Validate endpoint structure\n",
        "            for i, endpoint in enumerate(endpoints):\n",
        "                required_fields = ['name', 'url', 'expected_status']\n",
        "                missing_fields = [field for field in required_fields if field not in endpoint]\n",
        "\n",
        "                if missing_fields:\n",
        "                    raise ValueError(f\"Endpoint {i}: missing fields {missing_fields}\")\n",
        "\n",
        "            print(f\"Loaded {len(endpoints)} endpoints from {filename}\")\n",
        "            return endpoints\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Endpoints file not found: {filename}\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Invalid JSON in endpoints file: {e}\")\n",
        "\n",
        "    async def check_endpoint_with_retry(self, session: aiohttp.ClientSession, endpoint: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Check a single endpoint with retry logic and exponential backoff.\"\"\"\n",
        "        name = endpoint['name']\n",
        "        url = endpoint['url']\n",
        "        expected_status = endpoint['expected_status']\n",
        "\n",
        "        start_time = time.time()\n",
        "        attempts = 0\n",
        "        last_error = None\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            attempts += 1\n",
        "\n",
        "            try:\n",
        "                # Calculate backoff delay: 2^attempt seconds (1, 2, 4, 8...)\n",
        "                if attempt > 0:\n",
        "                    backoff_delay = 2 ** (attempt - 1)\n",
        "                    print(f\"   {name}: Retry {attempt} after {backoff_delay}s delay\")\n",
        "                    await asyncio.sleep(backoff_delay)\n",
        "\n",
        "                # Make HTTP request\n",
        "                async with session.get(url, timeout=aiohttp.ClientTimeout(total=self.timeout)) as response:\n",
        "                    response_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "\n",
        "                    result = {\n",
        "                        'name': name,\n",
        "                        'url': url,\n",
        "                        'final_status_code': response.status,\n",
        "                        'ok': response.status == expected_status,\n",
        "                        'response_time_ms': round(response_time, 2),\n",
        "                        'attempts': attempts,\n",
        "                        'error': None\n",
        "                    }\n",
        "\n",
        "                    if result['ok']:\n",
        "                        print(f\" {name}: OK ({response.status}) in {response_time:.1f}ms\")\n",
        "                        return result\n",
        "                    else:\n",
        "                        last_error = f\"Status {response.status}, expected {expected_status}\"\n",
        "                        if attempt < self.max_retries - 1:\n",
        "                            print(f\" {name}: {last_error}, retrying...\")\n",
        "                            continue\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                last_error = f\"Timeout after {self.timeout}s\"\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    print(f\" {name}: {last_error}, retrying...\")\n",
        "                    continue\n",
        "\n",
        "            except aiohttp.ClientError as e:\n",
        "                last_error = f\"Connection error: {str(e)[:100]}\"\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    print(f\" {name}: {last_error}, retrying...\")\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                last_error = f\"Unexpected error: {str(e)[:100]}\"\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    print(f\" {name}: {last_error}, retrying...\")\n",
        "                    continue\n",
        "\n",
        "        # All retries failed\n",
        "        response_time = (time.time() - start_time) * 1000\n",
        "        result = {\n",
        "            'name': name,\n",
        "            'url': url,\n",
        "            'final_status_code': None,\n",
        "            'ok': False,\n",
        "            'response_time_ms': round(response_time, 2),\n",
        "            'attempts': attempts,\n",
        "            'error': last_error\n",
        "        }\n",
        "\n",
        "        print(f\" {name}: FAILED after {attempts} attempts - {last_error}\")\n",
        "        return result\n",
        "\n",
        "    async def check_all_endpoints(self, endpoints: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Check all endpoints concurrently.\"\"\"\n",
        "        print(f\"\\n Starting health checks for {len(endpoints)} endpoints...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create HTTP session with connection pooling\n",
        "        connector = aiohttp.TCPConnector(limit=10, limit_per_host=3)\n",
        "        timeout = aiohttp.ClientTimeout(total=self.timeout)\n",
        "\n",
        "        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
        "            # Run all health checks concurrently\n",
        "            tasks = [self.check_endpoint_with_retry(session, endpoint) for endpoint in endpoints]\n",
        "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "            # Handle any exceptions that weren't caught\n",
        "            processed_results = []\n",
        "            for i, result in enumerate(results):\n",
        "                if isinstance(result, Exception):\n",
        "                    endpoint = endpoints[i]\n",
        "                    processed_results.append({\n",
        "                        'name': endpoint['name'],\n",
        "                        'url': endpoint['url'],\n",
        "                        'final_status_code': None,\n",
        "                        'ok': False,\n",
        "                        'response_time_ms': 0,\n",
        "                        'attempts': 1,\n",
        "                        'error': f\"Unhandled exception: {str(result)}\"\n",
        "                    })\n",
        "                else:\n",
        "                    processed_results.append(result)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n  Total execution time: {total_time:.2f}s\")\n",
        "\n",
        "        self.results = processed_results\n",
        "        return processed_results\n",
        "\n",
        "    def save_results_csv(self, filename: str):\n",
        "        \"\"\"Save results to CSV file.\"\"\"\n",
        "        fieldnames = ['name', 'url', 'final_status_code', 'ok', 'response_time_ms', 'attempts', 'error']\n",
        "\n",
        "        with open(filename, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(self.results)\n",
        "\n",
        "        print(f\" Results saved to CSV: {filename}\")\n",
        "\n",
        "    def save_results_json(self, filename: str):\n",
        "        \"\"\"Save results to JSON file.\"\"\"\n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'summary': {\n",
        "                'total_endpoints': len(self.results),\n",
        "                'successful': sum(1 for r in self.results if r['ok']),\n",
        "                'failed': sum(1 for r in self.results if not r['ok']),\n",
        "                'avg_response_time_ms': round(sum(r['response_time_ms'] for r in self.results) / len(self.results), 2) if self.results else 0\n",
        "            },\n",
        "            'results': self.results\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        print(f\" Results saved to JSON: {filename}\")\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print summary of health check results.\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to summarize\")\n",
        "            return\n",
        "\n",
        "        successful = [r for r in self.results if r['ok']]\n",
        "        failed = [r for r in self.results if not r['ok']]\n",
        "\n",
        "        print(f\"\\n HEALTH CHECK SUMMARY\")\n",
        "        print(f\"Total endpoints: {len(self.results)}\")\n",
        "        print(f\" Successful: {len(successful)}\")\n",
        "        print(f\" Failed: {len(failed)}\")\n",
        "\n",
        "        if successful:\n",
        "            avg_response_time = sum(r['response_time_ms'] for r in successful) / len(successful)\n",
        "            print(f\" Average response time: {avg_response_time:.1f}ms\")\n",
        "\n",
        "        if failed:\n",
        "            print(f\"\\n Failed endpoints:\")\n",
        "            for result in failed:\n",
        "                print(f\"  - {result['name']}: {result['error']}\")\n",
        "\n",
        "# Test the health checker\n",
        "async def main():\n",
        "    print(\"=== CONCURRENT REST HEALTH CHECKER ===\")\n",
        "\n",
        "    checker = HealthChecker(timeout=5, max_retries=3)\n",
        "\n",
        "    # Create sample endpoints\n",
        "    endpoints = checker.create_sample_endpoints('endpoints.json')\n",
        "\n",
        "    # Load endpoints and run health checks\n",
        "    endpoints = checker.load_endpoints('endpoints.json')\n",
        "    results = await checker.check_all_endpoints(endpoints)\n",
        "\n",
        "    # Save results and print summary\n",
        "    checker.save_results_csv('health_check_results.csv')\n",
        "    checker.save_results_json('health_check_results.json')\n",
        "    checker.print_summary()\n",
        "\n",
        "# Run the async main function\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a22898",
      "metadata": {
        "id": "96a22898"
      },
      "source": [
        "### Key Async and HTTP Methods:\n",
        "\n",
        "**Async Programming Fundamentals:**\n",
        "- **`async def function()`** - Define asynchronous function\n",
        "- **`await expression`** - Wait for async operation to complete\n",
        "- **`asyncio.gather(*tasks)`** - Run multiple async functions concurrently\n",
        "- **`asyncio.sleep(seconds)`** - Non-blocking delay (vs `time.sleep()`)\n",
        "\n",
        "**HTTP Client Operations:**\n",
        "- **`aiohttp.ClientSession()`** - HTTP client with connection pooling\n",
        "- **`session.get(url, timeout=timeout)`** - Async HTTP GET request\n",
        "- **`aiohttp.ClientTimeout(total=seconds)`** - Request timeout configuration\n",
        "- **`aiohttp.TCPConnector(limit=n)`** - Connection pool limits\n",
        "\n",
        "**Error Handling and Retries:**\n",
        "- **Exponential backoff** - `2 ** (attempt - 1)` for progressive delays\n",
        "- **`aiohttp.ClientError`** - Base exception for HTTP client errors\n",
        "- **`asyncio.TimeoutError`** - Exception for request timeouts\n",
        "- **`return_exceptions=True`** - Handle exceptions in `asyncio.gather()`\n",
        "\n",
        "**Performance Measurement:**\n",
        "- **`time.time()`** - High-precision timestamp for response time calculation\n",
        "- **Concurrent execution** - Multiple requests in parallel vs sequential\n",
        "- **Connection pooling** - Reuse HTTP connections for efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95a6e49",
      "metadata": {
        "id": "f95a6e49"
      },
      "source": [
        "## Section 6: Text Processing and Package Development\n",
        "\n",
        "### Theory: Text Normalization Pipelines\n",
        "\n",
        "Text processing is fundamental for:\n",
        "- **Log analysis** - Extracting insights from unstructured log data\n",
        "- **Configuration processing** - Normalizing config file formats\n",
        "- **Documentation generation** - Processing README and help text\n",
        "- **Data preprocessing** - Cleaning text data for analysis\n",
        "\n",
        "### Essential Text Processing Methods:\n",
        "- **`str.lower()`** - Convert text to lowercase\n",
        "- **`re.sub(pattern, replacement, text)`** - Replace text patterns with regex\n",
        "- **`str.split()`** - Split text into tokens\n",
        "- **`collections.Counter()`** - Count occurrences of items\n",
        "- **`str.join(iterable)`** - Join text elements with separator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e50ebb9",
      "metadata": {
        "id": "3e50ebb9",
        "outputId": "1de26c6a-7503-4e1a-ed73-36c93cca3d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TEXT NORMALIZATION TOOLKIT ===\n",
            "Created sample stopwords file: custom_stopwords.txt\n",
            "\n",
            "Original text (225 chars):\n",
            "\"This is a SAMPLE text with   multiple    spaces!!! \n",
            "    It has punctuation, UPPERCASE letters, and other issues.\n",
            "    We need to normalize this text for processing... \n",
            "    Docker containers and Kubernetes services are running.\"\n",
            "\n",
            "=== STEP-BY-STEP NORMALIZATION ===\n",
            "1. Normalize whitespace: \"This is a SAMPLE text with multiple spaces!!! It has punctuation, UPPERCASE letters, and other issues. We need to normalize this text for processing... Docker containers and Kubernetes services are running.\"\n",
            "2. Remove punctuation: \"This is a SAMPLE text with multiple spaces It has punctuation UPPERCASE letters and other issues We need to normalize this text for processing Docker containers and Kubernetes services are running\"\n",
            "3. Lowercase: \"this is a sample text with multiple spaces it has punctuation uppercase letters and other issues we need to normalize this text for processing docker containers and kubernetes services are running\"\n",
            "4. Tokenize: ['this', 'is', 'a', 'sample', 'text', 'with', 'multiple', 'spaces', 'it', 'has', 'punctuation', 'uppercase', 'letters', 'and', 'other', 'issues', 'we', 'need', 'to', 'normalize', 'this', 'text', 'for', 'processing', 'docker', 'containers', 'and', 'kubernetes', 'services', 'are', 'running']\n",
            "5. Word counts: {'this': 2, 'sample': 1, 'text': 2, 'multiple': 1, 'spaces': 1, 'it': 1, 'punctuation': 1, 'uppercase': 1, 'letters': 1, 'other': 1}\n",
            "\n",
            "=== COMPLETE PIPELINE RESULTS ===\n",
            "{\n",
            "  \"original_length\": 225,\n",
            "  \"normalized_length\": 196,\n",
            "  \"total_tokens\": 31,\n",
            "  \"unique_words_count\": 20,\n",
            "  \"top_10_words\": {\n",
            "    \"this\": 2,\n",
            "    \"text\": 2,\n",
            "    \"sample\": 1,\n",
            "    \"multiple\": 1,\n",
            "    \"spaces\": 1,\n",
            "    \"it\": 1,\n",
            "    \"punctuation\": 1,\n",
            "    \"uppercase\": 1,\n",
            "    \"letters\": 1,\n",
            "    \"other\": 1\n",
            "  },\n",
            "  \"processed_text\": \"this is a sample text with multiple spaces it has punctuation uppercase letters and other issues we need to normalize this text for processing docker containers and kubernetes services are running\"\n",
            "}\n",
            "\n",
            "=== TESTING CUSTOM STOPWORDS ===\n",
            "Loaded 25 custom stopwords from custom_stopwords.txt\n",
            "With custom stopwords: 10 unique words\n",
            "Top words: ['this', 'text', 'sample', 'multiple', 'spaces']\n"
          ]
        }
      ],
      "source": [
        "# Example 7: Text Normalization Toolkit\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Set\n",
        "\n",
        "class TextNormalizer:\n",
        "    \"\"\"Comprehensive text normalization toolkit for DevOps text processing.\"\"\"\n",
        "\n",
        "    def __init__(self, stopwords: Set[str] = None):\n",
        "        \"\"\"Initialize with optional custom stopwords set.\"\"\"\n",
        "        self.default_stopwords = {\n",
        "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',\n",
        "            'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n",
        "            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
        "            'will', 'would', 'could', 'should', 'may', 'might', 'can'\n",
        "        }\n",
        "        self.stopwords = stopwords if stopwords is not None else self.default_stopwords\n",
        "\n",
        "    def normalize_whitespace(self, text: str) -> str:\n",
        "        \"\"\"Normalize whitespace by collapsing multiple spaces and removing leading/trailing space.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(\"Input must be a string\")\n",
        "\n",
        "        # Replace multiple whitespace characters with single space\n",
        "        normalized = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove leading and trailing whitespace\n",
        "        return normalized.strip()\n",
        "\n",
        "    def remove_punctuation(self, text: str) -> str:\n",
        "        \"\"\"Remove punctuation while preserving word boundaries.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(\"Input must be a string\")\n",
        "\n",
        "        # Remove punctuation but keep word boundaries\n",
        "        # This pattern keeps alphanumeric characters and spaces\n",
        "        cleaned = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def lowercase(self, text: str) -> str:\n",
        "        \"\"\"Convert text to lowercase.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(\"Input must be a string\")\n",
        "\n",
        "        return text.lower()\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text into words using whitespace and basic punctuation.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(\"Input must be a string\")\n",
        "\n",
        "        # Split on whitespace and filter out empty strings\n",
        "        tokens = [token for token in text.split() if token]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def unique_word_counts(self, tokens: List[str], stopwords: Set[str] = None) -> Dict[str, int]:\n",
        "        \"\"\"Count unique words, optionally removing stopwords.\"\"\"\n",
        "        if not isinstance(tokens, list):\n",
        "            raise TypeError(\"Tokens must be a list\")\n",
        "\n",
        "        if stopwords is None:\n",
        "            stopwords = self.stopwords\n",
        "\n",
        "        # Filter out stopwords and count occurrences\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "        return dict(Counter(filtered_tokens))\n",
        "\n",
        "    def process_text_pipeline(self, text: str, remove_stopwords: bool = True) -> Dict[str, any]:\n",
        "        \"\"\"Complete text processing pipeline with all normalization steps.\"\"\"\n",
        "        original_length = len(text)\n",
        "\n",
        "        # Step 1: Normalize whitespace\n",
        "        normalized = self.normalize_whitespace(text)\n",
        "\n",
        "        # Step 2: Remove punctuation\n",
        "        no_punct = self.remove_punctuation(normalized)\n",
        "\n",
        "        # Step 3: Convert to lowercase\n",
        "        lowercased = self.lowercase(no_punct)\n",
        "\n",
        "        # Step 4: Tokenize\n",
        "        tokens = self.tokenize(lowercased)\n",
        "\n",
        "        # Step 5: Count unique words\n",
        "        word_counts = self.unique_word_counts(tokens, self.stopwords if remove_stopwords else set())\n",
        "\n",
        "        # Get top 10 words\n",
        "        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "        return {\n",
        "            'original_length': original_length,\n",
        "            'normalized_length': len(lowercased),\n",
        "            'total_tokens': len(tokens),\n",
        "            'unique_words_count': len(word_counts),\n",
        "            'top_10_words': dict(top_words),\n",
        "            'processed_text': lowercased\n",
        "        }\n",
        "\n",
        "    def load_stopwords_file(self, filename: str) -> Set[str]:\n",
        "        \"\"\"Load custom stopwords from file (one word per line).\"\"\"\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                stopwords = {line.strip().lower() for line in f if line.strip()}\n",
        "\n",
        "            print(f\"Loaded {len(stopwords)} custom stopwords from {filename}\")\n",
        "            self.stopwords = stopwords\n",
        "            return stopwords\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Stopwords file not found: {filename}. Using default stopwords.\")\n",
        "            return self.stopwords\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading stopwords file: {e}. Using default stopwords.\")\n",
        "            return self.stopwords\n",
        "\n",
        "    def create_sample_stopwords_file(self, filename: str):\n",
        "        \"\"\"Create a sample stopwords file for testing.\"\"\"\n",
        "        sample_stopwords = [\n",
        "            '# Common English stopwords',\n",
        "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',\n",
        "            'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were',\n",
        "            '# Technical stopwords for DevOps',\n",
        "            'server', 'service', 'docker', 'kubernetes', 'config'\n",
        "        ]\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            for word in sample_stopwords:\n",
        "                f.write(word + '\\n')\n",
        "\n",
        "        print(f\"Created sample stopwords file: {filename}\")\n",
        "\n",
        "# Test the text normalizer\n",
        "print(\"=== TEXT NORMALIZATION TOOLKIT ===\")\n",
        "\n",
        "# Create normalizer instance\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Create sample stopwords file\n",
        "normalizer.create_sample_stopwords_file('custom_stopwords.txt')\n",
        "\n",
        "# Test individual functions\n",
        "sample_text = \"\"\"\n",
        "    This is a SAMPLE text with   multiple    spaces!!!\n",
        "    It has punctuation, UPPERCASE letters, and other issues.\n",
        "    We need to normalize this text for processing...\n",
        "    Docker containers and Kubernetes services are running.\n",
        "\"\"\".strip()\n",
        "\n",
        "print(f\"\\nOriginal text ({len(sample_text)} chars):\")\n",
        "print(f'\"{sample_text}\"')\n",
        "\n",
        "# Test normalization steps\n",
        "print(\"\\n=== STEP-BY-STEP NORMALIZATION ===\")\n",
        "\n",
        "step1 = normalizer.normalize_whitespace(sample_text)\n",
        "print(f\"1. Normalize whitespace: \\\"{step1}\\\"\")\n",
        "\n",
        "step2 = normalizer.remove_punctuation(step1)\n",
        "print(f\"2. Remove punctuation: \\\"{step2}\\\"\")\n",
        "\n",
        "step3 = normalizer.lowercase(step2)\n",
        "print(f\"3. Lowercase: \\\"{step3}\\\"\")\n",
        "\n",
        "step4 = normalizer.tokenize(step3)\n",
        "print(f\"4. Tokenize: {step4}\")\n",
        "\n",
        "step5 = normalizer.unique_word_counts(step4)\n",
        "print(f\"5. Word counts: {dict(list(step5.items())[:10])}\")  # Show first 10\n",
        "\n",
        "# Test complete pipeline\n",
        "print(\"\\n=== COMPLETE PIPELINE RESULTS ===\")\n",
        "result = normalizer.process_text_pipeline(sample_text)\n",
        "\n",
        "print(json.dumps(result, indent=2))\n",
        "\n",
        "# Test with custom stopwords\n",
        "print(\"\\n=== TESTING CUSTOM STOPWORDS ===\")\n",
        "normalizer.load_stopwords_file('custom_stopwords.txt')\n",
        "result_custom = normalizer.process_text_pipeline(sample_text)\n",
        "\n",
        "print(f\"With custom stopwords: {len(result_custom['top_10_words'])} unique words\")\n",
        "print(f\"Top words: {list(result_custom['top_10_words'].keys())[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f193a2",
      "metadata": {
        "id": "49f193a2"
      },
      "source": [
        "### Key Text Processing Methods:\n",
        "\n",
        "**Regular Expression Operations:**\n",
        "- **`re.sub(pattern, replacement, text)`** - Replace all matches of pattern with replacement\n",
        "- **`r'\\\\s+'`** - Raw string pattern matching one or more whitespace characters\n",
        "- **`r'[^\\\\w\\\\s]'`** - Pattern matching non-word, non-space characters (punctuation)\n",
        "- **Character classes** - `\\\\w` (word chars), `\\\\s` (whitespace), `^` (negation)\n",
        "\n",
        "**String Processing:**\n",
        "- **`str.split()`** - Split string on whitespace into list of tokens\n",
        "- **`str.strip()`** - Remove leading and trailing whitespace\n",
        "- **`str.lower()`** - Convert string to lowercase for normalization\n",
        "- **List comprehensions** - `[token for token in tokens if condition]` for filtering\n",
        "\n",
        "**Data Structures for Counting:**\n",
        "- **`collections.Counter(iterable)`** - Count occurrences of hashable objects\n",
        "- **`dict(Counter(items))`** - Convert Counter to regular dictionary\n",
        "- **`sorted(items, key=lambda x: x[1], reverse=True)`** - Sort by count descending\n",
        "- **Set operations** - `token in stopwords` for efficient membership testing\n",
        "\n",
        "**Type Safety and Validation:**\n",
        "- **`isinstance(obj, type)`** - Runtime type checking for function inputs\n",
        "- **Type hints** - `Set[str]`, `List[str]`, `Dict[str, int]` for documentation\n",
        "- **Input validation** - Raising `TypeError` for invalid input types\n",
        "- **Default parameters** - Using `None` with conditional assignment for optional params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf55cce",
      "metadata": {
        "id": "7cf55cce"
      },
      "source": [
        "## Homework Assignment: Complete DevOps Automation Tasks\n",
        "\n",
        "### 🎯 **Task 1: Log Analyzer CLI Tool**\n",
        "\n",
        "**Objective**: Build a professional command-line log analyzer that processes web server logs efficiently.\n",
        "\n",
        "**Requirements**:\n",
        "- Create `log_analyzer.py` with `argparse` CLI interface\n",
        "- Support options: `--input`, `--top-n`, `--output-json`, `--output-tsv`\n",
        "- Implement streaming file processing (no full file load)\n",
        "- Calculate: total requests, status code distribution (2xx/3xx/4xx/5xx), top N client IPs, top N paths, hourly counts\n",
        "- Handle malformed lines gracefully (log and skip)\n",
        "- Write unit tests with `pytest`\n",
        "\n",
        "**Deliverables**:\n",
        "```bash\n",
        "python log_analyzer.py --input large_log.log --top-n 10 --output-json report.json --output-tsv report.tsv\n",
        "```\n",
        "\n",
        "**Acceptance Criteria**:\n",
        "- Tool processes 100k+ lines without memory issues\n",
        "- JSON output contains all required metrics\n",
        "- TSV file opens correctly in Excel\n",
        "- Tests cover edge cases and malformed lines\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Task 2: Configuration Tool Package**\n",
        "\n",
        "**Objective**: Create an installable Python package for YAML configuration validation.\n",
        "\n",
        "**Requirements**:\n",
        "- Package structure with `setup.py` or `pyproject.toml`\n",
        "- CLI tool `cfgtool` that validates YAML configs\n",
        "- Validate fields: name, version, services list\n",
        "- Provide `requirements.txt` and `bootstrap.sh` script\n",
        "- Include 4+ unit tests for config parsing and error handling\n",
        "\n",
        "**Deliverables**:\n",
        "```bash\n",
        "pip install -e .\n",
        "cfgtool config.yml  # Prints validated summary\n",
        "```\n",
        "\n",
        "**Package Structure**:\n",
        "```\n",
        "cfgtool/\n",
        "├── setup.py\n",
        "├── requirements.txt\n",
        "├── bootstrap.sh\n",
        "├── cfgtool/\n",
        "│   ├── __init__.py\n",
        "│   ├── cli.py\n",
        "│   └── validator.py\n",
        "└── tests/\n",
        "    └── test_cfgtool.py\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Task 3: Configuration-Driven File Renamer**\n",
        "\n",
        "**Objective**: Build a safe file renaming tool with dry-run capability and rollback support.\n",
        "\n",
        "**Requirements**:\n",
        "- YAML config defines glob patterns → rename templates\n",
        "- CLI options: `--config`, `--dir`, `--dry-run`, `--commit`, `--force`\n",
        "- Template variables: `{date}`, `{datetime}`, `{orig}`, `{ext}`, `{parent}`\n",
        "- Conflict detection and prevention\n",
        "- Generate `rename_manifest.json` for rollback\n",
        "- Unit tests for pattern matching and conflict detection\n",
        "\n",
        "**Example Usage**:\n",
        "```bash\n",
        "python renamer.py --config rules.yml --dir /logs --dry-run\n",
        "python renamer.py --config rules.yml --dir /logs --commit\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Task 4: CSV Aggregator → Excel Report**\n",
        "\n",
        "**Objective**: Process multiple CSV transaction files into comprehensive Excel reports.\n",
        "\n",
        "**Requirements**:\n",
        "- Input: folder with CSV files (timestamp, user_id, service, amount columns)\n",
        "- Aggregate by day and service: count, sum, average amounts\n",
        "- Output Excel with 3 sheets: DailySummary, ServiceSummary, RawMerged\n",
        "- Use `pandas` and `openpyxl` for processing\n",
        "- Include data validation script\n",
        "- Format Excel sheets professionally\n",
        "\n",
        "**Expected Output**:\n",
        "- Multi-sheet Excel workbook\n",
        "- Proper formatting and column headers\n",
        "- Validation that aggregates match raw data\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Task 5: Concurrent REST Health Checker**\n",
        "\n",
        "**Objective**: Implement async HTTP health monitoring with retry logic.\n",
        "\n",
        "**Requirements**:\n",
        "- JSON config: `[{\"name\": \"...\", \"url\": \"...\", \"expected_status\": 200}]`\n",
        "- Use `asyncio` and `aiohttp` for concurrent requests\n",
        "- Retry logic: 3 attempts with exponential backoff\n",
        "- Output CSV/JSON with: name, url, status_code, ok, response_time_ms, attempts\n",
        "- Unit tests with mocked HTTP responses\n",
        "- Performance comparison vs sequential execution\n",
        "\n",
        "**Key Features**:\n",
        "- Exponential backoff: 1s, 2s, 4s delays\n",
        "- Connection pooling for efficiency\n",
        "- Timeout handling and error reporting\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Task 6: Text Normalization Package & CLI**\n",
        "\n",
        "**Objective**: Create a reusable text processing package with CLI interface.\n",
        "\n",
        "**Requirements**:\n",
        "- Package `textnorm` with functions:\n",
        "  - `normalize_whitespace(text)`\n",
        "  - `remove_punctuation(text)`\n",
        "  - `lowercase(text)`\n",
        "  - `tokenize(text)`\n",
        "  - `unique_word_counts(tokens, stopwords=[])`\n",
        "- CLI reads from file/stdin, outputs JSON summary\n",
        "- Support custom stopwords file\n",
        "- Include type hints and comprehensive tests\n",
        "\n",
        "**CLI Output Example**:\n",
        "```json\n",
        "{\n",
        "  \"original_length\": 150,\n",
        "  \"normalized_length\": 145,\n",
        "  \"unique_words_count\": 25,\n",
        "  \"top_10_words\": {\"python\": 5, \"devops\": 3, \"automation\": 2}\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📋 **Submission Guidelines**\n",
        "\n",
        "**Project Structure**:\n",
        "```\n",
        "session1_homework/\n",
        "├── task1_log_analyzer/\n",
        "│   ├── log_analyzer.py\n",
        "│   ├── tests/\n",
        "│   └── sample_data/\n",
        "├── task2_cfgtool/\n",
        "│   ├── setup.py\n",
        "│   ├── cfgtool/\n",
        "│   └── tests/\n",
        "├── task3_renamer/\n",
        "├── task4_report_builder/\n",
        "├── task5_health_checker/\n",
        "├── task6_textnorm/\n",
        "├── requirements.txt\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "**Quality Requirements**:\n",
        "- ✅ All code includes error handling\n",
        "- ✅ Unit tests with >80% coverage\n",
        "- ✅ Type hints where appropriate\n",
        "- ✅ Documentation strings\n",
        "- ✅ CLI help messages\n",
        "- ✅ Professional logging output\n",
        "\n",
        "**Testing Commands**:\n",
        "```bash\n",
        "# Run all tests\n",
        "python -m pytest tests/ -v --cov\n",
        "\n",
        "# Test individual tasks\n",
        "python -m pytest tests/test_log_analyzer.py -v\n",
        "python -m pytest tests/test_cfgtool.py -v\n",
        "```\n",
        "\n",
        "### 🏆 **Success Criteria**\n",
        "\n",
        "Each task will be evaluated on:\n",
        "1. **Functionality** - Meets all requirements\n",
        "2. **Code Quality** - Clean, readable, well-structured\n",
        "3. **Error Handling** - Graceful failure modes\n",
        "4. **Testing** - Comprehensive test coverage\n",
        "5. **Documentation** - Clear usage instructions\n",
        "6. **Performance** - Efficient for expected workloads\n",
        "\n",
        "**Due Date**: [Insert appropriate deadline]\n",
        "**Submission**: Git repository with complete implementation and README"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77bcf4a6",
      "metadata": {
        "id": "77bcf4a6"
      },
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### What We've Learned\n",
        "\n",
        "In this advanced session, we covered essential DevOps automation techniques:\n",
        "\n",
        "1. **CLI Development** - Professional command-line tools with `argparse`\n",
        "2. **File Processing** - Memory-efficient streaming and pattern matching\n",
        "3. **Package Development** - Creating installable Python packages\n",
        "4. **Data Pipelines** - ETL operations with pandas and Excel output\n",
        "5. **Concurrent Programming** - Async HTTP operations with proper error handling\n",
        "6. **Text Processing** - Comprehensive normalization and analysis pipelines\n",
        "\n",
        "### Key DevOps Principles Applied\n",
        "\n",
        "**Automation First**\n",
        "- Every manual task should be automated\n",
        "- Tools should be reusable and configurable\n",
        "- Error handling must be comprehensive\n",
        "\n",
        "**Performance Considerations**\n",
        "- Stream large files instead of loading into memory\n",
        "- Use async operations for I/O-bound tasks\n",
        "- Implement connection pooling and retry logic\n",
        "\n",
        "**Reliability and Safety**\n",
        "- Dry-run modes for destructive operations\n",
        "- Comprehensive logging and error reporting\n",
        "- Rollback capabilities for file operations\n",
        "- Input validation and type checking\n",
        "\n",
        "**Professional Development**\n",
        "- Complete test coverage with pytest\n",
        "- Type hints for better code documentation\n",
        "- CLI interfaces with proper help messages\n",
        "- Package structure following Python conventions\n",
        "\n",
        "### Production Deployment Checklist\n",
        "\n",
        "✅ **Code Quality**\n",
        "- Error handling for all failure modes\n",
        "- Comprehensive logging at appropriate levels\n",
        "- Input validation and sanitization\n",
        "- Type hints and documentation\n",
        "\n",
        "✅ **Testing**\n",
        "- Unit tests for all functions\n",
        "- Integration tests for complete workflows\n",
        "- Mocked external dependencies\n",
        "- Performance tests for large datasets\n",
        "\n",
        "✅ **Documentation**\n",
        "- Clear README with usage examples\n",
        "- API documentation for functions\n",
        "- CLI help messages\n",
        "- Error message clarity\n",
        "\n",
        "✅ **Deployment**\n",
        "- Requirements.txt with pinned versions\n",
        "- Virtual environment setup scripts\n",
        "- Configuration file examples\n",
        "- Installation and setup instructions\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "These tools form the foundation for more advanced DevOps automation:\n",
        "\n",
        "- **CI/CD Integration** - Incorporating tools into pipeline automation\n",
        "- **Monitoring and Alerting** - Using health checkers for system monitoring\n",
        "- **Infrastructure as Code** - Configuration-driven infrastructure management\n",
        "- **Observability** - Log analysis and metrics collection at scale\n",
        "\n",
        "Practice building and deploying these tools in real environments to gain experience with production considerations like security, scalability, and maintainability."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
